{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FomSnGil9EcS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zac9ykANMRND"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "rawdata_1 = pd.read_excel('/content/drive/MyDrive/Datamining/LFB_2019-22_south.xlsx')\n",
        "#original data is close to 400k rows\n",
        "#for my baroughb it is 18k\n",
        "meta_data1 = pd.read_excel('/content/drive/MyDrive/Datamining/LFB Metadata.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNuBe672UnLS"
      },
      "source": [
        "#Initial data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9ofTatxMaky"
      },
      "outputs": [],
      "source": [
        "#only cover numarical columns\n",
        "rawdata_1.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm83gJClhg87"
      },
      "outputs": [],
      "source": [
        "rawdata_1.describe(include = 'O')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7WYS0HEky_4"
      },
      "outputs": [],
      "source": [
        "#get information about a method from offitial library\n",
        "rawdata_1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onuMDceMhn6C"
      },
      "outputs": [],
      "source": [
        "rawdata_1[rawdata_1['CalYear']==2021].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5EwVA7hmiKX"
      },
      "outputs": [],
      "source": [
        "#percantage of missing values\n",
        "print(\"percantage of missingvalues\")\n",
        "(rawdata_1.isna().sum().sort_values(ascending=False)/len(rawdata_1)*100).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzI_Vrzyn406"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.imshow(rawdata_1.isnull(), cmap='hot', aspect='auto')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZdkkZRTNe5q"
      },
      "outputs": [],
      "source": [
        "rawdata_1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-iXs4AQauaa"
      },
      "outputs": [],
      "source": [
        "#change data type\n",
        "\n",
        "#if three time on value does not mean three time imporatant\n",
        "#it is catgrical\n",
        "\n",
        "rawdata_1['CalYear'] = rawdata_1['CalYear'].astype('O')\n",
        "rawdata_1['CalYear'].dtype # Changed .type to .dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q-is8P5NF1r"
      },
      "outputs": [],
      "source": [
        "rawdata_1.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvbALuMoUqzk"
      },
      "source": [
        "##columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDzfEmanSiW4"
      },
      "outputs": [],
      "source": [
        "columns1=['IncidentNumber', 'DateOfCall', 'CalYear', 'TimeOfCall', 'HourOfCall',\n",
        "       'IncidentGroup', 'StopCodeDescription', 'SpecialServiceType',\n",
        "       'PropertyCategory', 'PropertyType', 'AddressQualifier', 'Postcode_full',\n",
        "       'Postcode_district', 'UPRN', 'USRN', 'IncGeo_BoroughCode',\n",
        "       'IncGeo_BoroughName', 'ProperCase', 'IncGeo_WardCode',\n",
        "       'IncGeo_WardName', 'IncGeo_WardNameNew', 'Easting_m', 'Northing_m',\n",
        "       'Easting_rounded', 'Northing_rounded', 'Latitude', 'Longitude', 'FRS',\n",
        "       'IncidentStationGround', 'FirstPumpArriving_AttendanceTime',\n",
        "       'FirstPumpArriving_DeployedFromStation',\n",
        "       'SecondPumpArriving_AttendanceTime',\n",
        "       'SecondPumpArriving_DeployedFromStation',\n",
        "       'NumStationsWithPumpsAttending', 'NumPumpsAttending', 'PumpCount',\n",
        "       'PumpHoursRoundUp', 'Notional Cost (£)', 'NumCalls']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P-5xjP36UXhO"
      },
      "outputs": [],
      "source": [
        "#dont display, too many values\n",
        "cat_columns2 = [\n",
        "    'IncidentNumber' ,\n",
        "    'DateOfCall',#Data of call\n",
        "    'TimeOfCall',#time of the call\n",
        "    'UPRN', #Unique Property Reference Number\n",
        "    'USRN', #Unique Street Reference Number\n",
        "    'Postcode_full',#full poste code\n",
        "    ]\n",
        "\n",
        "cat_columns1 = [\n",
        "    'CalYear','HourOfCall',\n",
        "    'IncidentGroup',#general type of insidante\n",
        "    'StopCodeDescription',#detaile type of incidante or false alarm\n",
        "    'SpecialServiceType',#if type was spacial service, what happened\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    'AddressQualifier',#does given adress same as fire\n",
        "    'Postcode_district',#post district\n",
        "    'IncGeo_WardCode', 'IncGeo_WardNameNew',#ward codes + name + new name\n",
        "    'IncidentStationGround',#fire brigaded area\n",
        "    'FirstPumpArriving_DeployedFromStation',#what station send first\n",
        "    'SecondPumpArriving_DeployedFromStation',#second pump location\n",
        "    'NumStationsWithPumpsAttending',#number of stations with pump\n",
        "    'NumPumpsAttending',#number of pumps attending\n",
        "    'PumpCount',#number of pumps\n",
        "    'NumCalls' #number of calls\n",
        "    ]\n",
        "\n",
        "drop_columns = [\n",
        "    #single value\n",
        "    #Borough Code #1 #Borough Name #1\n",
        "    'ProperCase',\n",
        "    'IncGeo_BoroughCode',\n",
        "    'IncGeo_BoroughName',\n",
        "    'FRS',#city name\n",
        "    #duplicate\n",
        "    'IncGeo_WardName',#ward name #we have new name\n",
        "    'Notional Cost (£)', #multipicaction of hours round up\n",
        "    ]\n",
        "    #repeted columns or single values\n",
        "\n",
        "num_columns1 = [\n",
        "    'Easting_m',#X converted from longitude\n",
        "    'Northing_m',#Y converted from lattitude\n",
        "    'Easting_rounded',#X rounded\n",
        "    'Northing_rounded',#Y rounded\n",
        "    'Latitude',#lattitude\n",
        "    'Longitude',#longtitude\n",
        "    'FirstPumpArriving_AttendanceTime',#arival time of first pump\n",
        "    'SecondPumpArriving_AttendanceTime',#arival time of second pump\n",
        "    'PumpHoursRoundUp',#pump hours\n",
        "    ]\n",
        "    #numrical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Zb2v0DT9hfCI"
      },
      "outputs": [],
      "source": [
        "for i in cat_columns1:\n",
        "  print(i)\n",
        "  print(rawdata_1[i].unique())\n",
        "for i in cat_columns2:\n",
        "  print(i)\n",
        "  print(rawdata_1[i].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3XGOipn2hXcO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "lm = LabelEncoder\n",
        "data_2 = rawdata_1.copy()\n",
        "\n",
        "# Encoding categorical columns (optional, if needed)\n",
        "# for i in cat_columns1:\n",
        "#     data_2[i] = lm().fit_transform(data_2[i])\n",
        "\n",
        "# Define grid layout\n",
        "num_cols = 3  # Number of plots per row\n",
        "num_rows = (len(cat_columns1) + num_cols - 1) // num_cols  # Calculate number of rows\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 50))  # Adjust size as needed\n",
        "axes = axes.flatten()  # Flatten axes to easily index\n",
        "\n",
        "# Loop through columns and plot\n",
        "for idx, i in enumerate(cat_columns1):\n",
        "    data_2[i].value_counts().plot(kind='bar', ax=axes[idx])  # Assign plot to specific axis\n",
        "    axes[idx].set_xlabel(None)  # Hide x-axis label\n",
        "    axes[idx].set_ylabel(\"Count\")\n",
        "    axes[idx].set_title(f\"Bar plot {i}\")\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(idx + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oMG5_5JrTfas"
      },
      "outputs": [],
      "source": [
        "data_2[num_columns1].hist(figsize=(15, 12), bins=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8dC6yWdtuqY"
      },
      "outputs": [],
      "source": [
        "#in clustring k mean is linner-\n",
        "#we cannt use catgrical becuase it assime it is numerical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3XqZC25JIqq"
      },
      "source": [
        "#1 -- Use supervised learning (classification) to predict whether a call is likely to be a false alarm based on\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ4ydrMrDg9s"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "#creat 3 datasets for three questions\n",
        "data_1 = rawdata_1.copy()\n",
        "#: Use supervised learning (classification) to predict whether a call is likely to be a false alarm based on\n",
        "col_ned = [\n",
        "    #call info\n",
        "    'DateOfCall',#Data of call\n",
        "    'TimeOfCall',#time of the call\n",
        "    # 'CalYear','HourOfCall',\n",
        "    'NumCalls', #number of calls\n",
        "\n",
        "    #output\n",
        "    'IncidentGroup',#general type of insidante\n",
        "    # 'StopCodeDescription',#detaile type of incidante or false alarm\n",
        "    #property info\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    #location info\n",
        "\n",
        "    'Postcode_district',#post district\n",
        "    'IncGeo_WardCode', 'IncGeo_WardNameNew',#ward codes + name + new name\n",
        "]\n",
        "\n",
        "col_2 = [\n",
        "    'Easting_rounded',\n",
        "    'Northing_rounded'\n",
        "    # 'Latitude',#lattitude\n",
        "    # 'Longitude',#longtitude\n",
        "    # 'Postcode_full',#full poste code\n",
        "]\n",
        "creat_col=[\n",
        "    #season of call\n",
        "    'Day','Month','Year',\n",
        "    'Hour','Minute','Second'\n",
        "\n",
        "    #history of the ward\n",
        "\n",
        "    #District\n",
        "]\n",
        "#\n",
        "data_1[col_ned[0]][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef0g9dBeUw-e"
      },
      "source": [
        "##data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lC4sly4HxsQM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot of Longitude vs Latitude\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data_1[\"Longitude\"], y=data_1[\"Latitude\"], alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"Geographical Mapping of Incidents\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wsE1PXJ63MPb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot of Longitude vs Latitude\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data_1[\"Easting_rounded\"], y=data_1[\"Northing_rounded\"], alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Easting_rounded\")\n",
        "plt.ylabel(\"Northing_rounded\")\n",
        "plt.title(\"Geographical Mapping of Incidents\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s_Gbiwt13tvU"
      },
      "outputs": [],
      "source": [
        "data_1[['Easting_rounded','Northing_rounded']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J0DDBDjx3Ty"
      },
      "outputs": [],
      "source": [
        "data_ = data_1.drop(data_1[(data_1['Easting_rounded'] > 550000) & (data_1['Northing_rounded'] > 200000)].index)\n",
        "data_ = data_.dropna(subset=['Easting_rounded', 'Northing_rounded'])\n",
        "data_.shape\n",
        "# data_ = data_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q418ilBP2xQS"
      },
      "outputs": [],
      "source": [
        "# rawdata_1[rawdata_1['Longitude']>-0.01 & rawdata_1['Latitude']<5]\n",
        "# data_ = data_1.drop(data_1[(data_1['Longitude'] > -0.01) & (data_1['Latitude'] < 5)].index)\n",
        "# data_ = data_.dropna(subset=['Longitude', 'Latitude'])\n",
        "#too manhy nulls\n",
        "# rawdata_1[(rawdata_1['Longitude']>-0.01) & (rawdata_1['Latitude']<5)][['Longitude','Latitude']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BpncAzUkyDz"
      },
      "outputs": [],
      "source": [
        "for col in col_ned[2:]:  # Assuming these are numerical columns\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=data_[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CRM22bLVAJE"
      },
      "outputs": [],
      "source": [
        "Q3 = data_['NumCalls'].quantile(0.999)\n",
        "if Q3<20: Q3=15\n",
        "data_ = data_[data_['NumCalls'] <= Q3]\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x=data_['NumCalls'])\n",
        "plt.title(f\"Boxplot of NumCalls\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0syl2CUDkRY1"
      },
      "source": [
        "##Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U_M9OoTZ4iNy"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import pyproj\n",
        "\n",
        "raw1 = 'NumCalls'\n",
        "unique_years = data_[raw1].unique()  # Get unique years\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate colors\n",
        "rows_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "# Define projection transformation (example: British National Grid to WGS 84)\n",
        "proj = pyproj.Transformer.from_crs(\"EPSG:27700\", \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "# Create the Folium map object before the loop\n",
        "m = folium.Map(location=[data_[\"Latitude\"].mean(), data_[\"Longitude\"].mean()], zoom_start=10) #added folium.Map object\n",
        "\n",
        "# Iterate over rows and create markers\n",
        "for _, row in data_.iterrows():\n",
        "    if pd.notna(row[\"Easting_rounded\"]) and pd.notna(row[\"Northing_rounded\"]):  # Check for NaN values\n",
        "        # Convert Easting/Northing to Latitude/Longitude\n",
        "        lon, lat = proj.transform(row[\"Easting_rounded\"], row[\"Northing_rounded\"])\n",
        "\n",
        "        row_ = row[raw1]  # Fix the indentation and variable reference\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[lat, lon],  # Use transformed coordinates\n",
        "            radius=1.5,\n",
        "            color=rows_color_map.get(row_, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=rows_color_map.get(row_, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m) #changed from .add_to(proj) to .add_to(m)\n",
        "m #displaying map instead of transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MM6EZLMvzhpQ"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate a color map for years\n",
        "raw1 = 'NumCalls'\n",
        "unique_years = data_[raw1].unique()  # Get unique years\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate colors\n",
        "rows_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "# ['Easting_rounded', 'Northing_rounded']\n",
        "# Center map at the mean coordinates\n",
        "m = folium.Map(location=[data_[\"Latitude\"].mean(), data_[\"Longitude\"].mean()], zoom_start=13)\n",
        "\n",
        "# Add points to the map\n",
        "for _, row in data_.iterrows():\n",
        "    if pd.notna(row[\"Latitude\"]) and pd.notna(row[\"Longitude\"]):  # Check for NaN values\n",
        "          row_ = row[raw1]\n",
        "        # if row_ != unique_years[0]:\n",
        "          folium.CircleMarker(\n",
        "            location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "            radius=1.5,\n",
        "            color=rows_color_map.get(row_, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=rows_color_map.get(row_, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "          ).add_to(m)\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IVwC1-bsaLCG"
      },
      "outputs": [],
      "source": [
        "pip install folium osmnx geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "L61IfrEXaIVZ"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import osmnx as ox\n",
        "import geopandas as gpd\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Define the area (adjust to your city or region)\n",
        "place_name = \"London, UK\"  # Change this to your target city/region\n",
        "\n",
        "# Get road network (only major roads: motorways, primary, secondary, and trunk)\n",
        "road_types = [\"motorway\", \"primary\", \"secondary\", \"trunk\"]\n",
        "roads = ox.graph_from_place(place_name, network_type=\"drive\")\n",
        "\n",
        "# Convert to GeoDataFrame\n",
        "edges = ox.graph_to_gdfs(roads, nodes=False, edges=True)\n",
        "main_roads = edges[edges['highway'].apply(lambda x: isinstance(x, list) and any(r in x for r in road_types) or x in road_types)]\n",
        "\n",
        "# Create base map centered around incident locations\n",
        "map_center = [data_['Latitude'].mean(), data_['Longitude'].mean()]\n",
        "incident_map = folium.Map(location=map_center, zoom_start=12)\n",
        "\n",
        "data_clean = data_.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "\n",
        "# Add heatmap for incidents\n",
        "heat_data = list(zip(data_clean['Latitude'], data_clean['Longitude'], data_clean['NumCalls']))\n",
        "HeatMap(heat_data, radius=10).add_to(incident_map)\n",
        "\n",
        "# Add main roads to the map\n",
        "for _, road in main_roads.iterrows():\n",
        "    road_coords = list(road.geometry.coords)\n",
        "    folium.PolyLine(road_coords, color=\"blue\", weight=3, opacity=0.8).add_to(incident_map)\n",
        "\n",
        "# Show map\n",
        "incident_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FAemMmTC2_sf"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate a color map for years\n",
        "row1 = 'PropertyCategory'\n",
        "unique_years = sorted(data_[row1].dropna().unique())  # Get unique years and sort them\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate distinct colors\n",
        "year_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "\n",
        "# Center the map at the mean coordinates\n",
        "m = folium.Map(location=[data_[\"Latitude\"].mean(), data_[\"Longitude\"].mean()], zoom_start=10)\n",
        "\n",
        "# Add points to the map\n",
        "for _, row in data_.iterrows():\n",
        "    if pd.notna(row[\"Latitude\"]) and pd.notna(row[\"Longitude\"]):  # Check for NaN values\n",
        "        year = row[row1]\n",
        "        folium.CircleMarker(\n",
        "            location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "            radius=3,\n",
        "            color=year_color_map.get(year, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=year_color_map.get(year, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m)\n",
        "\n",
        "# Create a legend (color guide)\n",
        "legend_html = '''\n",
        "<div style=\"\n",
        "    position: fixed;\n",
        "    bottom: 50px; left: 50px; width: 200px; height: auto;\n",
        "    background-color: white; z-index:9999; font-size:14px;\n",
        "    padding: 10px; border-radius: 5px; box-shadow: 2px 2px 5px rgba(0,0,0,0.3);\n",
        "\">\n",
        "    <b>Year Color Guide</b><br>\n",
        "'''\n",
        "\n",
        "# Add each year to the legend with its corresponding color\n",
        "for year, color in year_color_map.items():\n",
        "    legend_html += f'<div style=\"display: flex; align-items: center;\"><div style=\"width: 15px; height: 15px; background-color: {color}; margin-right: 5px; border: 1px solid black;\"></div> {year}</div>'\n",
        "\n",
        "legend_html += \"</div>\"\n",
        "\n",
        "# Add the legend to the map\n",
        "m.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "# Add a title to the map\n",
        "title_html = '''\n",
        "<h3 align=\"center\" style=\"font-size:16px\"><b>Incident Map Colored by Year</b></h3>\n",
        "'''\n",
        "m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RdesirYIux2-"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate a color map for years\n",
        "row1 = 'PropertyType'\n",
        "unique_years = sorted(data_[row1].dropna().unique())  # Get unique years and sort them\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate distinct colors\n",
        "year_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "\n",
        "# Center the map at the mean coordinates\n",
        "m = folium.Map(location=[data_[\"Latitude\"].mean(), data_[\"Longitude\"].mean()], zoom_start=13)\n",
        "\n",
        "# Add points to the map\n",
        "for _, row in data_.iterrows():\n",
        "    if pd.notna(row[\"Latitude\"]) and pd.notna(row[\"Longitude\"]):  # Check for NaN values\n",
        "        year = row[row1]\n",
        "        folium.CircleMarker(\n",
        "            location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "            radius=1.5,\n",
        "            color=year_color_map.get(year, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=year_color_map.get(year, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m)\n",
        "\n",
        "# Create a legend (color guide)\n",
        "legend_html = '''\n",
        "<div style=\"\n",
        "    position: fixed;\n",
        "    bottom: 50px; left: 50px; width: 200px; height: auto;\n",
        "    background-color: white; z-index:9999; font-size:14px;\n",
        "    padding: 10px; border-radius: 5px; box-shadow: 2px 2px 5px rgba(0,0,0,0.3);\n",
        "\">\n",
        "    <b>Year Color Guide</b><br>\n",
        "'''\n",
        "\n",
        "# Add each year to the legend with its corresponding color\n",
        "for year, color in year_color_map.items():\n",
        "    legend_html += f'<div style=\"display: flex; align-items: center;\"><div style=\"width: 15px; height: 15px; background-color: {color}; margin-right: 5px; border: 1px solid black;\"></div> {year}</div>'\n",
        "\n",
        "legend_html += \"</div>\"\n",
        "\n",
        "# Add the legend to the map\n",
        "m.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "# Add a title to the map\n",
        "title_html = '''\n",
        "<h3 align=\"center\" style=\"font-size:16px\"><b>Incident Map Colored by Year</b></h3>\n",
        "'''\n",
        "m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgE1a-XrKjQO"
      },
      "outputs": [],
      "source": [
        "#creat data information\n",
        "data_[[\"Hour\", \"Minute\", \"Second\"]] = data_[\"TimeOfCall\"].str.split(\":\", expand=True)\n",
        "# Convert to datetime format\n",
        "data_[\"DateOfCall\"] = pd.to_datetime(data_[\"DateOfCall\"], format=\"%d %b %Y\")\n",
        "\n",
        "# Extract Day, Month, and Year\n",
        "data_[\"Day\"] = data_[\"DateOfCall\"].dt.day\n",
        "data_[\"Month\"] = data_[\"DateOfCall\"].dt.month\n",
        "data_[\"Year\"] = data_[\"DateOfCall\"].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9J-hqzkPfTH"
      },
      "outputs": [],
      "source": [
        "data_[creat_col[0]][1]\n",
        "# creat_col[0]\n",
        "# data_1[\"Day\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRjSApt8J6pw"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(data_[\"IncidentGroup\"], data_[\"HourOfCall\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrknEUmtkvhJ"
      },
      "source": [
        "##Pre model graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbyq8QGaJJo1"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# for i in creat_col:\n",
        "#   sns.violinplot(x=\"IncidentGroup\", y=i, data=data_1)\n",
        "#   plt.title(\"Boxplot of \"+i+\" by IncidentGroup\")\n",
        "#   plt.show()\n",
        "# Number of plots\n",
        "num_plots = len(creat_col)\n",
        "\n",
        "# Create subplots with 1 row and multiple columns\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))  # Adjust size\n",
        "\n",
        "# If only one column, axes won't be an array, so we wrap it\n",
        "if num_plots == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Loop through columns and create violin plots\n",
        "for i, col in enumerate(creat_col):\n",
        "    sns.violinplot(x=\"IncidentGroup\", y=col, data=data_, ax=axes[i])\n",
        "    axes[i].set_title(f\"Violin Plot of {col} by IncidentGroup\")\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lb7sI_-SI5Y"
      },
      "outputs": [],
      "source": [
        "col = ['PropertyCategory',\n",
        "       'NumCalls','Postcode_district','IncGeo_WardCode']\n",
        "\n",
        "num_plots = len(col)\n",
        "\n",
        "# Create subplots with 1 row and multiple columns\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))  # Adjust size\n",
        "\n",
        "# If only one column, axes won't be an array, so we wrap it\n",
        "if num_plots == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Loop through columns and create violin plots\n",
        "for i, col in enumerate(col):\n",
        "    sns.violinplot(x=\"IncidentGroup\", y=col, data=data_, ax=axes[i])\n",
        "    axes[i].set_title(f\"Violin Plot of {col} by IncidentGroup\")\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO6tbY3YkIMO"
      },
      "outputs": [],
      "source": [
        "data_[col_ned].isnull().sum()\n",
        "data_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvNv4ycok1Bi"
      },
      "source": [
        "##scaling and getting data ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P7SLmKzaw99N"
      },
      "outputs": [],
      "source": [
        "data_.columns\n",
        "data_2 = data_[[\n",
        "        'IncidentGroup', 'PropertyCategory', #'PropertyType',\n",
        "        #'AddressQualifier',\n",
        "        'Postcode_district',\n",
        "        'IncGeo_WardCode', 'IncGeo_WardNameNew',\n",
        "        'Easting_rounded',\n",
        "        'Northing_rounded',\n",
        "        'NumCalls', 'Hour', #'Minute',\n",
        "        #'Second', 'Day',\n",
        "        'Month', #'Year'\n",
        "        ]]\n",
        "# Select categorical columns\n",
        "categorical_cols = [\n",
        "    'IncidentGroup', 'PropertyCategory', 'Hour','Month',\n",
        "    'Postcode_district', 'IncGeo_WardCode', 'IncGeo_WardNameNew'\n",
        "]\n",
        "\n",
        "# Apply one-hot encoding\n",
        "data_2 = pd.get_dummies(data_2, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Check the transformed dataset\n",
        "print(data_2.head())\n",
        "# data_2['PropertyCategory'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWf6x0Yvjt6p"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_2[['Easting_rounded','Northing_rounded']] = scaler.fit_transform(data_2[['Easting_rounded','Northing_rounded']])  # Use encoded data from one-hot encoding\n",
        "\n",
        "target = data_2[['IncidentGroup_Fire']]\n",
        "# data_2['target ']  = data_2['IncidentGroup_Fire']\n",
        "data_2= data_2.drop(columns=['IncidentGroup_Fire','IncidentGroup_Special Service','PropertyCategory_Non Residential'])\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(data_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDd_I7mCxrGR"
      },
      "outputs": [],
      "source": [
        "# Explained variance ratio (how much information each component keeps)\n",
        "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Find number of components for 90% variance\n",
        "percent_1 = 0.80\n",
        "num_components = np.argmax(explained_variance >= percent_1) + 1\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axhline(y=percent_1, color='r', linestyle='-')  # 90% threshold line\n",
        "plt.axvline(x=num_components, color='g', linestyle='--')  # Vertical line at chosen components\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Explained Variance vs. Number of Components\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5LE9hgpyfXD"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA with 30 components\n",
        "pca = PCA(n_components=30)\n",
        "data_com = pca.fit_transform(data_2)\n",
        "\n",
        "# Convert back to a DataFrame (optional, if needed)\n",
        "data_com = pd.DataFrame(data_com, columns=[f'PC{i+1}' for i in range(30)])\n",
        "\n",
        "# Check the new compressed data\n",
        "print(data_com.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQMU132D5eJU"
      },
      "outputs": [],
      "source": [
        "target.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeTBqim0ykbI"
      },
      "source": [
        "##Modelsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxgKCA7cyjkv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "rn = RandomForestClassifier(n_estimators=40, random_state=42)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "#imbalacne data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(data_com, target)\n",
        "#test train split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "#train model\n",
        "rn.fit(x_train, y_train)\n",
        "#valication\n",
        "y_pred = rn.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy valication part: {accuracy}\")\n",
        "# rn.feature_importances_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW92aPRmtxD9"
      },
      "source": [
        "##validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQKzLKNH5zje"
      },
      "outputs": [],
      "source": [
        "y_pred = rn.predict(data_com)\n",
        "\n",
        "accuracy = accuracy_score(target, y_pred)\n",
        "print(f\"Accuracy overall: {accuracy}\")\n",
        "\n",
        "print(classification_report(target, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxSYxen0SdJd"
      },
      "outputs": [],
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Wrap Random Forest with a probability calibration method\n",
        "calibrated_model = CalibratedClassifierCV(rn, method=\"sigmoid\", cv=5)\n",
        "calibrated_model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UB4A7GDSssg"
      },
      "outputs": [],
      "source": [
        "# Get calibrated probabilities\n",
        "random_row = data_com.sample(n=1)  # Select one random row\n",
        "y_prob_calibrated = calibrated_model.predict_proba(random_row)\n",
        "print(y_prob_calibrated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MU9i4o-t0XB"
      },
      "outputs": [],
      "source": [
        "#look at where the false predictions are"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8B6k-6ZMMw5"
      },
      "source": [
        "#2- Supervised Regression Model: 1 - response time or  2 - Fire incident severity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xhoqL6oVoFW"
      },
      "source": [
        "##intro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK-mFQX3MGL8"
      },
      "outputs": [],
      "source": [
        "#creat 3 datasets for three questions\n",
        "data2_1 = rawdata_1.copy()\n",
        "#: Use supervised learning (classification) to predict whether a call is likely to be a false alarm based on\n",
        "col_ned2 = [\n",
        "    #call info\n",
        "    'DateOfCall',#Data of call\n",
        "    # 'CalYear','HourOfCall',\n",
        "    'NumCalls', #number of calls\n",
        "\n",
        "    #property info\n",
        "    #'IncidentGroup',#general type of incidante\n",
        "    'StopCodeDescription', #detaile type of incidante or false alarm\n",
        "    #'SpecialServiceType',\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    #'PropertyType', #info on type of building\n",
        "    'AddressQualifier', #was it correct address or nearby buiding\n",
        "    #location info\n",
        "    'IncGeo_WardCode',#ward codes#'IncGeo_WardNameNew',  + new name\n",
        "     #'Easting_m', 'Northing_m',\n",
        "    # 'Postcode_full', #full postcast + district\n",
        "    'Postcode_district', #post district\n",
        "    #target\n",
        "    #'IncidentStationGround',\n",
        "    'FirstPumpArriving_DeployedFromStation',\n",
        "]\n",
        "col_num2 = [\n",
        "    'TimeOfCall',#time of the call\n",
        "    'UPRN', #Unique Property Reference Number\n",
        "    'USRN', #Unique Street Reference Number\n",
        "    'Easting_rounded', 'Northing_rounded', #location on x and y\n",
        "    # 'Latitude', 'Longitude', #lattitude and lonitude\n",
        "    'FirstPumpArriving_AttendanceTime',#in seconds\n",
        "]\n",
        "creat_col2=[\n",
        "    #season of call\n",
        "    'Day','Month','Year',\n",
        "    'Hour','Minute','Second'\n",
        "]\n",
        "#Resource Allocation Factors: PropertyType, NumPumpsAttending, PumpHoursRoundUp\n",
        "data2_1[col_ned2].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnMLYXuLCq48"
      },
      "outputs": [],
      "source": [
        "# Calculate unique values per column and sum across columns\n",
        "for col in col_ned2:\n",
        "    unique_values = data2_1[col].nunique()  # Get the number of unique values\n",
        "    print(f'{col} : {unique_values}')  # Print the column name and its unique value count\n",
        "#decided to swipe PropertyType with PropertyCategory becuse it was too detailed\n",
        "#and data on PropertyCategory would give us the meanfult enough details\n",
        "#futher info can be studied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2nWAawzBrLS"
      },
      "source": [
        "##data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9p5Qti8yVvW"
      },
      "outputs": [],
      "source": [
        "data2_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbjk6rBYOeXr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot of Longitude vs Latitude\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data2_1[\"Easting_rounded\"], y=data2_1[\"Northing_rounded\"], alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Easting_rounded\")\n",
        "plt.ylabel(\"Northing_rounded\")\n",
        "plt.title(\"Geographical Mapping of Incidents\")\n",
        "plt.show()\n",
        "\n",
        "data2_2 = data2_1.drop(data2_1[(data2_1['Easting_rounded'] > 550000) & (data2_1['Northing_rounded'] > 200000)].index)\n",
        "# data2_2 = data2_1.dropna(subset=['Easting_rounded', 'Northing_rounded'])\n",
        "data2_2.shape\n",
        "# data_ = data_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOwebFufX85c"
      },
      "outputs": [],
      "source": [
        "#single outlier on nothing an esting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LagsMosmoUgQ"
      },
      "outputs": [],
      "source": [
        "print(data2_2.shape)\n",
        "data2_2.dropna(subset=['FirstPumpArriving_AttendanceTime'], inplace=True)\n",
        "print(data2_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQFlVbvFYBGJ"
      },
      "outputs": [],
      "source": [
        "#aroung 1500 outlier for first pump ariving time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3154cX0ocC6"
      },
      "outputs": [],
      "source": [
        "d3 = data2_2[(data2_2['UPRN']<0.7)]#(0.25*(10^11)))]# & (data2_1['USRN']<60)\n",
        "d4 = data2_2[(data2_2['USRN']<10e+06)]#1.8*(10^7))]\n",
        "\n",
        "sns.boxplot(x=data2_2['USRN'])\n",
        "plt.show()\n",
        "data2_2['USRN'].describe()\n",
        "# sns.boxplot(x=data2_1['UPRN'])\n",
        "# plt.show()\n",
        "# data2_1['UPRN'].describe()\n",
        "print(f'Number of UPRN outlier {d3.shape}')\n",
        "print(f'Number of USRN outlier {d4.shape}')\n",
        "data2_2 = data2_2.drop(data2_2[(data2_2['USRN']<10e+06)].index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veMfX_j_YQBN"
      },
      "outputs": [],
      "source": [
        " #cant use UPRN relaiably, too many 0 in it\n",
        " #data quilty issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un4M_qAcBBOw"
      },
      "outputs": [],
      "source": [
        "data2_2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSG9Q7PnILN-"
      },
      "outputs": [],
      "source": [
        "#creat data information\n",
        "data2_2[[\"Hour\", \"Minute\", \"Second\"]] = data2_2[\"TimeOfCall\"].str.split(\":\", expand=True)\n",
        "# Convert to datetime format\n",
        "data2_2[\"DateOfCall\"] = pd.to_datetime(data2_2[\"DateOfCall\"], format=\"%d %b %Y\")\n",
        "\n",
        "# Extract Day, Month, and Year\n",
        "data2_2[\"Day\"] = data2_2[\"DateOfCall\"].dt.day\n",
        "data2_2[\"Month\"] = data2_2[\"DateOfCall\"].dt.month\n",
        "data2_2[\"Year\"] = data2_2[\"DateOfCall\"].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_OauIbxJMVW"
      },
      "outputs": [],
      "source": [
        "#handle outlier of number of calls\n",
        "Q3 = data2_2['NumCalls'].quantile(0.999)\n",
        "if Q3<20: Q3=15\n",
        "data2_2 = data2_2[data2_2['NumCalls'] <= Q3]\n",
        "# plt.figure(figsize=(6, 4))\n",
        "# sns.boxplot(x=data2_2['NumCalls'])\n",
        "# plt.title(f\"Boxplot of NumCalls\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wX3FN1HFD5_"
      },
      "source": [
        "##pre graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bj54G9klCbob"
      },
      "outputs": [],
      "source": [
        "for col in col_ned2[2:]:  # Assuming these are numerical columns\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=data2_2[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()\n",
        "#we are going to ignore outlier of pump ariving time because it is out output\n",
        "#and they dont devate de results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Iou2xK_gOSZp"
      },
      "outputs": [],
      "source": [
        "data2_2[creat_col2].dtypes\n",
        "# data2_2[creat_col] = data2_2[creat_col].astype('O')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK_nOLriItjE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# Number of plots\n",
        "# data2_2[creat_col] = data2_2[creat_col].astype('O')\n",
        "# df[\"Month\"] = df[\"Month\"].astype(int)\n",
        "# df[\"Day\"] = df[\"Day\"].astype(int)\n",
        "\n",
        "num_plots = len(creat_col2)\n",
        "data2_2['arive'] = data2_2['FirstPumpArriving_AttendanceTime']/200\n",
        "# Create subplots with 1 row and multiple columns\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))  # Adjust size\n",
        "\n",
        "# If only one column, axes won't be an array, so we wrap it\n",
        "if num_plots == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Loop through columns and create violin plots\n",
        "for i, col in enumerate(creat_col2[0:3]):\n",
        "    sns.violinplot(x=col, y=\"arive\", data=data2_2, ax=axes[i])\n",
        "    axes[i].set_title(f\"Violin Plot of {col} by First Pump Time\")\n",
        "for i, col in enumerate(creat_col2[3:6]):\n",
        "    sns.violinplot(x=\"arive\", y=col, data=data2_2, ax=axes[i+3])\n",
        "    axes[i+3].set_title(f\"Violin Plot of {col} by First Pump Time\")\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTxTMFzsdnz8"
      },
      "source": [
        "----> this one dose not look nice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b83MVhF2P5k4"
      },
      "outputs": [],
      "source": [
        "col = [\n",
        "    'NumCalls', #number of calls\n",
        "    #property info\n",
        "    #'IncidentGroup',#general type of incidante\n",
        "    'StopCodeDescription', #detaile type of incidante or false alarm\n",
        "    #'SpecialServiceType',\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    #'PropertyType', #info on type of building\n",
        "    'AddressQualifier', #was it correct address or nearby buiding\n",
        "    #location info\n",
        "    'IncGeo_WardCode',#ward codes#'IncGeo_WardNameNew',  + new name\n",
        "     #'Easting_m', 'Northing_m',\n",
        "    # 'Postcode_full', #full postcast + district\n",
        "    'Postcode_district', #post district\n",
        "    ]\n",
        "\n",
        "num_plots = len(col)\n",
        "\n",
        "# Create subplots with 1 row and multiple columns\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))  # Adjust size\n",
        "\n",
        "# If only one column, axes won't be an array, so we wrap it\n",
        "if num_plots == 1:\n",
        "    axes = [axes]\n",
        "sns.violinplot(x=col[0], y=\"arive\", data=data2_2, ax=axes[0])\n",
        "axes[0].set_title(f\"Violin Plot of {col[0]} by IncidentGroup\")\n",
        "# Loop through columns and create violin plots\n",
        "for i, col in enumerate(col[1:]):\n",
        "    sns.violinplot(x=\"arive\", y=col, data=data2_2, ax=axes[i+1])\n",
        "    axes[i+1].set_title(f\"Violin Plot of {col} by IncidentGroup\")\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK8ZtHz_mnsD"
      },
      "source": [
        "##maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s77VKmYbmm0j"
      },
      "outputs": [],
      "source": [
        "import folium # Importing the folium library\n",
        "import pyproj  # Import the pyproj library\n",
        "\n",
        "raw1 = 'NumCalls'\n",
        "unique_years = data2_2[raw1].unique()  # Get unique years\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate colors\n",
        "rows_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "# Define projection transformation (example: British National Grid to WGS 84)\n",
        "proj = pyproj.Transformer.from_crs(\"EPSG:27700\", \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "# Create the Folium map object before the loop\n",
        "m = folium.Map(location=[data2_2[\"Latitude\"].mean(), data2_2[\"Longitude\"].mean()], zoom_start=11)\n",
        "#added folium.Map object\n",
        "\n",
        "# Iterate over rows and create markers\n",
        "for _, row in data2_2.iterrows():\n",
        "    if pd.notna(row[\"Easting_rounded\"]) and pd.notna(row[\"Northing_rounded\"]):\n",
        "       # Check for NaN values\n",
        "        # Convert Easting/Northing to Latitude/Longitude\n",
        "        lon, lat = proj.transform(row[\"Easting_rounded\"], row[\"Northing_rounded\"])\n",
        "\n",
        "        row_ = row[raw1]  # Fix the indentation and variable reference\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[lat, lon],  # Use transformed coordinates\n",
        "            radius=1.5,\n",
        "            color=rows_color_map.get(row_, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=rows_color_map.get(row_, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m) #changed from .add_to(proj) to .add_to(m)\n",
        "m #displaying map instead of transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlscekH9E4dD"
      },
      "source": [
        "##DATA preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEUS734Gf9_A"
      },
      "source": [
        "we are not using data an hour as a distingusier but rather a number so we consider then numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO6QLMxgBlj7"
      },
      "outputs": [],
      "source": [
        "col_ned2_2 = [\n",
        "    'StopCodeDescription',\n",
        "    'PropertyCategory','AddressQualifier',\n",
        "    'IncGeo_WardCode', 'Postcode_district',\n",
        "]\n",
        "col_num2_2 = [\n",
        "    # 'UPRN', #Unique Property Reference Number\n",
        "    # 'USRN', #Unique Street Reference Number\n",
        "    'Day','Month','Year',\n",
        "    'Hour','Minute','Second',\n",
        "    'NumCalls',\n",
        "    'Easting_rounded',\n",
        "    'Northing_rounded'\n",
        "    ]\n",
        "#UPRN and USRN add no meaning to the calcumaction as they are not repated,\n",
        "#and they are just and indicator\n",
        "#and they have too many differnt values which would make the data nosy\n",
        "\n",
        "# data2_2[col_num].isnull().sum()\n",
        "data2_2[col_ned2_2].isnull().sum()\n",
        "\n",
        "data2_3 = data2_2[col_ned2_2 + col_num2_2].copy()\n",
        "\n",
        "# Calculate unique values per column and sum across columns\n",
        "for col in col_ned2_2:\n",
        "    unique_values = data2_3[col].nunique()  # Get the number of unique values\n",
        "    print(f'{col} : {unique_values}')  # Print the column name and its unique value count\n",
        "\n",
        "print(data2_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPY8SAFXUEJk"
      },
      "outputs": [],
      "source": [
        "# data2_3.groupby(\"Northing_rounded\")[\"FirstPumpArriving_AttendanceTime\"].mean().sort_values()\n",
        "data2_3.columns\n",
        "data2_3.shape\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "data2_3 = pd.get_dummies(data2_3, columns=col_ned2_2, drop_first=True)\n",
        "# Apply one-hot encoding\n",
        "scaler = StandardScaler()\n",
        "# scaler = MinMaxScaler()\n",
        "data2_3[col_num2_2] = scaler.fit_transform(data2_3[col_num2_2])\n",
        "data2_3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0vKEpdH9gbSN"
      },
      "outputs": [],
      "source": [
        "data2_3.shape\n",
        "data2_3.columns\n",
        "for i in data2_3.columns:\n",
        "  print(f'columnd {i} : {data2_3[i].dtype}') # Change i.type to data2_3[i].dtype\n",
        "  # data2_3.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXnQRjonPGre"
      },
      "source": [
        "###pca application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qNDOpModTT2"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Check the transformed dataset\n",
        "# print(data_2.head())\n",
        "# scaler = StandardScaler()\n",
        "# data_2[['Easting_rounded','Northing_rounded']] = scaler.fit_transform(data_2[['Easting_rounded','Northing_rounded']])  # Use encoded data from one-hot encoding\n",
        "\n",
        "\n",
        "# data_2['target ']  = data_2['IncidentGroup_Fire']\n",
        "# data2_3= data2_3.drop(columns=['FirstPumpArriving_AttendanceTime'])\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(data2_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6Q-uYfyXuJb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Explained variance ratio (how much information each component keeps)\n",
        "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Find number of components for 90% variance\n",
        "percent_1 = 0.80\n",
        "num_components = np.argmax(explained_variance >= percent_1) + 1\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axhline(y=percent_1, color='r', linestyle='-')  # 90% threshold line\n",
        "\n",
        "plt.axvline(x=num_components, color='g', linestyle='--')  # Vertical line at chosen components\n",
        "\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Explained Variance vs. Number of Components\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnYUCNQQX3Hf"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA with 30 components\n",
        "pca2 = PCA(n_components=num_components)\n",
        "data2_com = pca2.fit_transform(data2_3)\n",
        "\n",
        "# Convert back to a DataFrame (optional, if needed)\n",
        "data2_com = pd.DataFrame(data2_com, columns=[f'PC{i+1}' for i in range(num_components)])\n",
        "\n",
        "# Check the new compressed data\n",
        "print(data2_com.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Bv2v8iYoll"
      },
      "source": [
        "##modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsCwf4ShYU40"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#no need for PCA\n",
        "target2 = data2_2['FirstPumpArriving_AttendanceTime'].copy()\n",
        "data2_com = data2_3\n",
        "\n",
        "# Split into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    data2_com, target2, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TVGwbqayicOZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "rn2 = RandomForestRegressor(n_estimators=100,\n",
        "                            min_samples_split=10, random_state=42)\n",
        "grid_search = GridSearchCV(rn2, param_grid, cv=5,\n",
        "                           n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(x_train, y_train)\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlbFx6ijies8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Validation\n",
        "y_pred = rn2.predict(x_test)\n",
        "\n",
        "# Regression metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Validation MAE: {mae}\")\n",
        "print(f\"Validation RMSE: {rmse}\")\n",
        "print(f\"Validation R² Score: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKrOM9EYY4yA"
      },
      "outputs": [],
      "source": [
        "# Predictions on the full dataset\n",
        "#\n",
        "y_pred_all = rn2.predict(data2_3)\n",
        "\n",
        "# Overall performance\n",
        "mae_all = mean_absolute_error(target2, y_pred_all)\n",
        "rmse_all = np.sqrt(mean_squared_error(target2, y_pred_all))\n",
        "r2_all = r2_score(target2, y_pred_all)\n",
        "\n",
        "print(f\"Overall MAE: {mae_all}\")\n",
        "print(f\"Overall RMSE: {rmse_all}\")\n",
        "print(f\"Overall R² Score: {r2_all}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "460HTUeNeVWU"
      },
      "source": [
        "##pos processing : checking accrucy across wards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljrtB7B6dqsv"
      },
      "outputs": [],
      "source": [
        "# #applying one hot encoding this way will relust in differnt numbero f\n",
        "# clumns as we reduce one of the catagrical types\n",
        "\n",
        "#thus making less culmns which will be problmadic with pca and model\n",
        "\n",
        "\n",
        "\n",
        "# unique_wards = data2_2[\"IncGeo_WardName\"].unique()\n",
        "\n",
        "# # Select two specific wards (replace with actual names)\n",
        "# ward_1 = unique_wards[0]\n",
        "# ward_2 = unique_wards[1]\n",
        "\n",
        "# # Filter the dataset for each ward\n",
        "# df_ward1 = data2_2[data2_2[\"IncGeo_WardName\"] == ward_1]\n",
        "# df_ward2 = data2_2[data2_2[\"IncGeo_WardName\"] == ward_2]\n",
        "\n",
        "# X_ward1 = pd.get_dummies(X_ward1, columns=col_ned, drop_first=True)\n",
        "# X_ward2 = pd.get_dummies(X_ward2, columns=col_ned, drop_first=True)\n",
        "\n",
        "# X_ward1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBmT397HiSET"
      },
      "outputs": [],
      "source": [
        "unique_wards = data2_2[\"IncGeo_WardCode\"].unique()\n",
        "df_ward1 = data2_2[col_ned2_2 + col_num2_2].copy()\n",
        "df_ward2 = pd.get_dummies(df_ward1, columns=col_ned2_2, drop_first=True)\n",
        "scaler = StandardScaler()\n",
        "# scaler = MinMaxScaler()\n",
        "data2_3[col_num2_2] = scaler.fit_transform(data2_3[col_num2_2])\n",
        "\n",
        "dy_ward1 = data2_2[\"FirstPumpArriving_AttendanceTime\"]\n",
        "\n",
        "# df_ward2 = pd.get_dummies(df_ward1, columns=col_ned2, drop_first=True)\n",
        "\n",
        "# Select two specific wards (replace with actual names)\n",
        "ward_1 = unique_wards[0]\n",
        "ward_2 = unique_wards[1]\n",
        "\n",
        "# Filter the dataset for each ward\n",
        "X_ward1 = df_ward2.loc[(df_ward1[\"IncGeo_WardCode\"] == ward_1), :]\n",
        "X_ward2 = df_ward2.loc[(df_ward1[\"IncGeo_WardCode\"] == ward_2), :]\n",
        "\n",
        "y_ward1 = dy_ward1[(df_ward1[\"IncGeo_WardCode\"] == ward_1)]\n",
        "y_ward2 = dy_ward1[(df_ward1[\"IncGeo_WardCode\"] == ward_2)]\n",
        "\n",
        "#applying one hot encoding this way will relust in differnt numbero f clumns as we reduce one of the catagrical types\n",
        "#thus making less culmns which will be problmadic with pca and model\n",
        "print(X_ward1.shape)\n",
        "# print(df_ward1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LnII8vHgP0R"
      },
      "outputs": [],
      "source": [
        "# X_ward1 = pca2.transform(X_ward1)\n",
        "# X_ward2 = pca2.transform(X_ward2)\n",
        "\n",
        "X_ward1 = X_ward1\n",
        "X_ward2 = X_ward2\n",
        "\n",
        "# Make predictions for both wards\n",
        "y_pred_ward1 = rn2.predict(X_ward1)\n",
        "y_pred_ward2 = rn2.predict(X_ward2)\n",
        "\n",
        "# Calculate error metrics for each ward\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
        "\n",
        "# Get results\n",
        "results_ward1 = evaluate_model(y_ward1, y_pred_ward1)\n",
        "results_ward2 = evaluate_model(y_ward2, y_pred_ward2)\n",
        "\n",
        "# Print the comparison\n",
        "print(f\"Performance in {ward_1}: {results_ward1}\")\n",
        "print(f\"Performance in {ward_2}: {results_ward2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDPyAhFVf8u-"
      },
      "outputs": [],
      "source": [
        "unique_wards = data2_2[\"Year\"].unique()\n",
        "print(unique_wards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDnspTPbBlYA"
      },
      "outputs": [],
      "source": [
        "#trend over the years and accuracy for years\n",
        "\n",
        "\n",
        "dy_ward1 = data2_2[\"FirstPumpArriving_AttendanceTime\"]\n",
        "\n",
        "# df_ward2 = pd.get_dummies(df_ward1, columns=col_ned2, drop_first=True)\n",
        "\n",
        "# Select two specific wards (replace with actual names)\n",
        "# ward_1 = unique_wards[0]\n",
        "for ward_1 in unique_wards:\n",
        "# Filter the dataset for each ward\n",
        "  X_ward1 = df_ward2.loc[(df_ward1[\"Year\"] == ward_1), :]\n",
        "# X_ward2 = df_ward2.loc[(df_ward1[\"IncGeo_WardCode\"] == ward_2), :]\n",
        "  y_ward1 = dy_ward1[(df_ward1[\"Year\"] == ward_1)]\n",
        "# y_ward2 = dy_ward1[(df_ward1[\"IncGeo_WardCode\"] == ward_2)]\n",
        "  y_pred_ward1 = rn2.predict(X_ward1)\n",
        "  print(f\"Performance in {ward_1}: {results_ward1}\")\n",
        "# y_pred_ward2 = rn2.predict(X_ward2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brS2uay4mtYp"
      },
      "source": [
        "#model 3 : Identify hidden patterns in fire incidents to adjust safety policies, station resources, and response strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q71D8WxHzSHP"
      },
      "source": [
        "##Defeonitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3EUwIN51msoR"
      },
      "outputs": [],
      "source": [
        "col_cat3 = [\n",
        "    #call info\n",
        "    # 'DateOfCall',#Data of call\n",
        "    # 'TimeOfCall',#time of the call\n",
        "    # 'CalYear','HourOfCall',\n",
        "\n",
        "    #property info\n",
        "    #'IncidentGroup',#general type of incidante\n",
        "    'StopCodeDescription', #detaile type of incidante or false alarm\n",
        "    #'SpecialServiceType',\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    #'PropertyType', #info on type of building\n",
        "    # 'ProperCase',\n",
        "    # 'AddressQualifier', #was it correct address or nearby buiding\n",
        "\n",
        "    #location info\n",
        "    'IncGeo_WardCode',#ward codes#'IncGeo_WardNameNew',  + new name\n",
        "    'Postcode_district', #post district\n",
        "\n",
        "    #deplyment\n",
        "    #'IncidentStationGround',\n",
        "]\n",
        "\n",
        "col_num3 = [\n",
        "    'NumCalls', #number of calls\n",
        "    'Easting_rounded', 'Northing_rounded', #location on x and y\n",
        "\n",
        "    'FirstPumpArriving_AttendanceTime',#in seconds\n",
        "    # 'FirstPumpArriving_DeployedFromStation',\n",
        "    'SecondPumpArriving_AttendanceTime',\n",
        "    # 'SecondPumpArriving_DeployedFromStation',\n",
        "    'NumStationsWithPumpsAttending',\n",
        "    'NumPumpsAttending',\n",
        "    'PumpHoursRoundUp',\n",
        "    # 'UPRN', #Unique Property Reference Number\n",
        "    # 'USRN', #Unique Street Reference Number\n",
        "    # 'Latitude', 'Longitude', #lattitude and lonitude\n",
        "    #'Easting_m', 'Northing_m',\n",
        "]\n",
        "\n",
        "data3_1 = rawdata_1.copy()\n",
        "data3_1[col_cat3+col_num3].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E5XOwztRfqc"
      },
      "outputs": [],
      "source": [
        "data3_1['ProperCase'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PO9QhU5-r6l"
      },
      "source": [
        "##data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JmWeL_kDIJg5"
      },
      "outputs": [],
      "source": [
        "#0 in SecondPumpArriving_AttendanceTime mean there was no arival\n",
        "data3_1['SecondPumpArriving_AttendanceTime'] = data3_1['SecondPumpArriving_AttendanceTime'].fillna(0)\n",
        "data3_1['FirstPumpArriving_AttendanceTime'] = data3_1['FirstPumpArriving_AttendanceTime'].fillna(0)\n",
        "data3_1 = data3_1.dropna(subset=['PumpHoursRoundUp'], inplace=False)\n",
        "\n",
        "data3_1[col_cat3+col_num3].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Kqji03YIpYLY"
      },
      "outputs": [],
      "source": [
        "data3_1 = data3_1.dropna(subset=['NumStationsWithPumpsAttending'], inplace=False)\n",
        "data3_1[col_cat3+col_num3].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ds5BbH7RADSx"
      },
      "outputs": [],
      "source": [
        "#Incident types (AFA false alarms, real fires, special services).\n",
        "#Location data (boroughs, wards, streets).\n",
        "#Time-based trends (seasonality, hourly variations).\n",
        "#Deployment time & resources needed (pump count, arrival times).\n",
        "for col in col_cat3:\n",
        "    unique_values = data3_1[col].nunique()  # Get the number of unique values\n",
        "    print(f'{col} : {unique_values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1IhqGZYeC8GH"
      },
      "outputs": [],
      "source": [
        "#checking for duplicated\n",
        "duplicates = data3_1[data3_1.duplicated()]\n",
        "print(duplicates)\n",
        "# data3_1.duplicated?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxVKY6-QkvNG"
      },
      "source": [
        "###graphs of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "n-gCkbaQ8mD7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot of Longitude vs Latitude\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data3_1[\"Easting_rounded\"], y=data3_1[\"Northing_rounded\"], alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Easting_rounded\")\n",
        "plt.ylabel(\"Northing_rounded\")\n",
        "plt.title(\"Geographical Mapping of Incidents\")\n",
        "plt.show()\n",
        "#intersting with thoese rows of null removed, the data dont need any cleaning with location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GovL-iUh7fHf"
      },
      "outputs": [],
      "source": [
        "for col in col_cat3[:-2 ]:  # Assuming these are numerical columns\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=data3_1[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()\n",
        "\n",
        "#we are going to ignore outlier of pump ariving time because it is out output\n",
        "#and they dont devate de results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ggzqbBhp9PQX"
      },
      "outputs": [],
      "source": [
        "for col in col_num3[:]:  # Assuming these are numerical columns\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=data3_1[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzt2jszqOx1R"
      },
      "outputs": [],
      "source": [
        "Q3 = data3_1['NumCalls'].quantile(0.999)\n",
        "if Q3<20: Q3=15\n",
        "data3_1 = data3_1[data3_1['NumCalls'] <= Q3]\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x=data3_1['NumCalls'])\n",
        "plt.title(f\"Boxplot of NumCalls\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ezn28mZwiZYS"
      },
      "outputs": [],
      "source": [
        "#PumpHoursRoundUp\n",
        "Q3 = data3_1['PumpHoursRoundUp'].quantile(0.99)\n",
        "if Q3<20: Q3=15\n",
        "d1 = data3_1[data3_1['PumpHoursRoundUp'] >= Q3]\n",
        "# d1 = data3_1[data3_1['PumpHoursRoundUp'] <= Q3]\n",
        "d1.shape\n",
        "d1.describe()\n",
        "# print(Q3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE72JrDJjwuT"
      },
      "source": [
        "only 27 incidents require more than 15 hours round at location --\n",
        "\n",
        "\n",
        "In them, first pump arrival was faster than usual by 5% -- so first pump arival did not contibute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrM4YYVdkBh7"
      },
      "outputs": [],
      "source": [
        "data3_1 = data3_1[data3_1['PumpHoursRoundUp'] <= Q3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZIedyPVO7Uu"
      },
      "source": [
        "##Preper Data for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "F5-QIWelU4uX"
      },
      "outputs": [],
      "source": [
        "data3_2 = data3_1[col_cat3+col_num3]\n",
        "\n",
        "# Apply one-hot encoding\n",
        "data3_2 = pd.get_dummies(data3_2, columns=col_cat3, drop_first=True)\n",
        "\n",
        "# Check the transformed dataset\n",
        "print(f'Data 3 Columns after one hot encoding : ')\n",
        "for i, column in enumerate(data3_2.columns):\n",
        "    print(f\"{i + 1}. {column}\",end = ' , ')\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs4J9ZVrO5LR"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "# Apply one-hot encoding\n",
        "# scaler = StandardScaler()\n",
        "scaler = MinMaxScaler()\n",
        "data3_2[col_num3] = scaler.fit_transform(data3_2[col_num3])\n",
        "data3_2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzs33lhWvSQa"
      },
      "source": [
        "##model 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acodbssD7IFX"
      },
      "source": [
        "###asstioation rule 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lNq_FxE_c3i"
      },
      "outputs": [],
      "source": [
        "#rules for scaled data\n",
        "\n",
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "# from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Convert categorical data into one-hot encoding (binary format)\n",
        "# te = TransactionEncoder()\n",
        "filtered_list = [item for item in data3_2.columns if item not in col_num3]\n",
        "\n",
        "\n",
        "# encoded_data = te.fit_transform(data3_2[filtered_list])\n",
        "# df = pd.DataFrame(data3_2, columns=filtered_list)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(data3_2[filtered_list], min_support=0.003, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
        "\n",
        "# Print results\n",
        "print(\"Frequent Itemsets:\\n\", frequent_itemsets)\n",
        "print(\"\\nAssociation Rules:\\n\", rules)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUkcfqvT1bQU"
      },
      "outputs": [],
      "source": [
        "# Filter rules with high confidence and high lift\n",
        "high_reliability_rules = rules[(rules['confidence'] > 0.8) & (rules['lift'] > 1.5)]\n",
        "\n",
        "# Print the filtered rules\n",
        "print(\"\\nHighly Reliable Association Rules:\\n\", high_reliability_rules)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-6cdy6v7FDV"
      },
      "source": [
        "###assoation rule 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Mcn3Ki3Jx6"
      },
      "source": [
        "several cloumns are very close in meaning\n",
        "\n",
        "IncGeo_WardCode_E05011096\n",
        "\n",
        "Postcode_district_SE5\n",
        "\n",
        "test to remove them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsS_erhZ3p-2"
      },
      "outputs": [],
      "source": [
        "\n",
        "data3_3 = data3_1[col_cat3+col_num3]\n",
        "data3_3 = pd.get_dummies(data3_3, columns=col_cat3[:-1], drop_first=True)\n",
        "# print(data3_3.shape)\n",
        "\n",
        "data3_3.drop(columns=col_cat3[-1], inplace=True)\n",
        "data3_3.drop(columns=col_num3, inplace=True)\n",
        "\n",
        "data3_3.columns\n",
        "\n",
        "# filtered_list2 = [item for item in data3_3.columns if item not in col_num3]\n",
        "frequent_itemsets2 = apriori(data3_3, min_support=0.003, use_colnames=True)\n",
        "rules2 = association_rules(frequent_itemsets2, metric=\"confidence\", min_threshold=0.5)\n",
        "# Filter rules with high confidence and high lift\n",
        "con = 0.6\n",
        "lif = 1.5\n",
        "high_reliability_rules2 = rules2[(rules2['confidence'] > con) & (rules2['lift'] > lif)]\n",
        "\n",
        "#\n",
        "print('confidance is how strange the given relation is in data')\n",
        "print('lift is how stronger given relation is is compre to random guess')\n",
        "\n",
        "# Print the filtered rules\n",
        "# print(\"\\nNew Highly Reliable Association Rules:\\n\", high_reliability_rules2)\n",
        "print(f'Rules with confidance > {con} and lift {lif} :')\n",
        "print('rule , (lift , confidance) : ')\n",
        "for rows in high_reliability_rules2.iloc:\n",
        "    antecedent_values = [item for item in rows['antecedents']]  # List comprehension\n",
        "    print(antecedent_values, end = ' , ')\n",
        "    # Access the 'lift' value directly, no need to iterate\n",
        "    lift_value = rows['lift']\n",
        "    print(\"({:.2f} , \".format(lift_value), end = ' ')\n",
        "    # Access the 'confidence' value directly\n",
        "    confidence_value = rows['confidence']\n",
        "    print(f'{confidence_value:.2f})')\n",
        "    # print(high_reliability_rules2['antecedents'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkC2vULwOdHN"
      },
      "outputs": [],
      "source": [
        "rules2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvXP4pO6SrVZ"
      },
      "source": [
        "####pos processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MEe9MfvEsJz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset (assuming `rules` is a DataFrame)\n",
        "data = rules2[['confidence', 'lift']]\n",
        "\n",
        "# Standardize the data for better clustering\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Apply K-Means clustering (try different `k` values)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "rules2['cluster'] = kmeans.fit_predict(data_scaled)\n",
        "\n",
        "# Scatter plot of clusters\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(rules2['confidence'], rules2['lift'], c=rules2['cluster'], cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('Confidence')\n",
        "plt.ylabel('Lift')\n",
        "plt.title('Clustering of Association Rules')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3DyvqgpMUey"
      },
      "outputs": [],
      "source": [
        "# Filter rules based on cluster\n",
        "cluster_1 = rules2[rules2['cluster'] == 2]  # Cluster 1: Strong Rules\n",
        "# cluster_2 = rules2[rules2['cluster'] == 1]  # Cluster 2: Moderate Rules\n",
        "# cluster_3 = rules2[rules2['cluster'] == 0]  # Cluster 3: Weak Rules\n",
        "\n",
        "# Example: Show top 3 rules from each cluster\n",
        "print(\"\\nCluster 1 (Strong and reliable Rules):\")\n",
        "print(cluster_1[['antecedents', 'consequents', 'confidence', 'lift']])\n",
        "\n",
        "# print(\"\\nCluster 2 (Strong but random Rules):\")\n",
        "# print(cluster_2[['antecedents', 'consequents', 'confidence', 'lift']].head(3))\n",
        "\n",
        "# print(\"\\nCluster 3 (Weak and unreliable Rules):\")\n",
        "# print(cluster_3[['antecedents', 'consequents', 'confidence', 'lift']].head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MvRycSxbJeC"
      },
      "source": [
        "##general Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg8UuANMrUrV"
      },
      "outputs": [],
      "source": [
        "#we have catagrocal values, should not use distance base cacluactions like Kmean\n",
        "#DBSCAN cant have negative input, so no standrad scaler\n",
        "\n",
        "#to handle the mixture of catgrical and numarical values, we have to do something\n",
        "#using gower made no change to the data, jut took very long to caluclate\n",
        "!pip install gower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_8x8UPabNdI"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Assuming data3_2 is your feature data for clustering\n",
        "\n",
        "# kmeans2 = KMeans(n_clusters=6, random_state=42)\n",
        "# # Fit and predict on data3_2 to get cluster labels\n",
        "# cluster_labels = kmeans2.fit_predict(data3_2)\n",
        "from sklearn.cluster import DBSCAN\n",
        "# import gower\n",
        "\n",
        "# Compute Gower distance\n",
        "# gower_dist = gower.gower_matrix(data3_2)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)#, metric=\"precomputed\")\n",
        "clusters = dbscan.fit_predict(data3_2)#gower_dist)\n",
        "\n",
        "# Assign cluster labels back to the dataset\n",
        "data3_2['Cluster'] = clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3_XnNebkPha"
      },
      "outputs": [],
      "source": [
        "for val in data3_1['StopCodeDescription'].unique():\n",
        "  print(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vhwfy0SeHkU"
      },
      "outputs": [],
      "source": [
        "#need to return data to original format to see next part\n",
        "#getting clusters back to orignal format\n",
        "categorical_columns = ['StopCodeDescription', 'PropertyCategory', 'Postcode_district', 'IncGeo_WardCode']\n",
        "data3_6 = data3_2.copy()\n",
        "for colmn1 in categorical_columns:\n",
        "  stop_code_unique_values = [col for col in data3_2.columns if col.startswith(colmn1 + '_')]\n",
        "  data3_6[colmn1] = data3_2[stop_code_unique_values].idxmax(axis=1).str.replace(colmn1+'_', '')\n",
        "# colmn1= 'StopCodeDescription'\n",
        "data3_6['Cluster'] = clusters\n",
        "\n",
        "# data3_6['StopCodeDescription'] = data3_2[\n",
        "    # ['StopCodeDescription_' + val for val in data3_1['StopCodeDescription'].unique()]\n",
        "    # ].idxmax(axis=1).str.replace('StopCodeDescription_', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6TMfsTnSdhIx"
      },
      "outputs": [],
      "source": [
        "data3_6['Cluster'] = clusters\n",
        "# Explore the data grouped by cluster\n",
        "cluster_summary = data3_6.groupby('Cluster').agg({\n",
        "    'StopCodeDescription': 'unique',\n",
        "    'PropertyCategory': 'unique',\n",
        "    'Postcode_district': 'unique',\n",
        "    'NumCalls': 'mean',\n",
        "    'Easting_rounded': 'mean',\n",
        "    'Northing_rounded': 'mean',\n",
        "    'FirstPumpArriving_AttendanceTime': 'mean',\n",
        "    'SecondPumpArriving_AttendanceTime': 'mean',\n",
        "    'NumStationsWithPumpsAttending': 'mean',\n",
        "    'NumPumpsAttending': 'mean',\n",
        "    'PumpHoursRoundUp': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Display the cluster summary to see what each cluster represents\n",
        "print(cluster_summary)\n",
        "\n",
        "# You can also look at a few example rows from each cluster\n",
        "cluster_examples = data3_6.groupby('Cluster').apply(lambda x: x.head(3))  # Show top 3 rows per cluster\n",
        "print(cluster_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjMaPw18lVBP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scatter plot to visualize clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "col1 = 'NumCalls'#'PumpHoursRoundUp'\n",
        "col2 = 'Postcode_district'\n",
        "# plt.scatter(data3_6['Easting_rounded'], data3_6['Northing_rounded'], c=data3_6['Cluster'])# cmap='viridis', alpha=0.7)\n",
        "plt.scatter(data3_6[col1], data3_6[col2], c=data3_6['Cluster'])# cmap='viridis', alpha=0.7)\n",
        "plt.xlabel(col1)\n",
        "plt.ylabel(col2)\n",
        "plt.title('Cluster Distribution of Fire Incidents')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bTily6fWpZaJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data3_6' contains the cluster labels in the 'Cluster' column\n",
        "\n",
        "# Loop through each unique cluster and plot it\n",
        "unique_clusters = data3_6['Cluster'].unique()\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    # Filter the data for the current cluster\n",
        "    cluster_data = data3_6[data3_6['Cluster'] == cluster]\n",
        "\n",
        "    # Create a scatter plot for the current cluster\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(cluster_data['Easting_rounded'], cluster_data['Northing_rounded'], alpha=0.7)\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.xlabel('Easting_rounded')\n",
        "    plt.ylabel('Northing_rounded')\n",
        "    plt.title(f'Cluster {cluster} Distribution of Fire Incidents')\n",
        "    plt.colorbar(label='Cluster')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvxXDRaSlnmm"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "heatmap_data = data3_6.pivot_table(index='StopCodeDescription', columns='PropertyCategory', values='Cluster', aggfunc='count')\n",
        "\n",
        "sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='g', linewidths=0.5)\n",
        "plt.title('Cluster Distribution Heatmap')\n",
        "plt.xlabel('Property Category')\n",
        "plt.ylabel('Stop Code Description')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZsNj-rYnxMb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "cluster_counts = data3_6.groupby(['StopCodeDescription', 'Cluster']).size().unstack()\n",
        "\n",
        "cluster_counts.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='viridis')\n",
        "\n",
        "plt.title('Cluster Distribution Across StopCodeDescription')\n",
        "plt.xlabel('Stop Code Description')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UNuBe672UnLS",
        "q3XqZC25JIqq",
        "Ef0g9dBeUw-e",
        "nrknEUmtkvhJ",
        "kvNv4ycok1Bi",
        "jW92aPRmtxD9",
        "O2nWAawzBrLS",
        "OK8ZtHz_mnsD",
        "uXnQRjonPGre",
        "Q71D8WxHzSHP",
        "6PO9QhU5-r6l",
        "BxVKY6-QkvNG",
        "acodbssD7IFX",
        "V-6cdy6v7FDV",
        "yvXP4pO6SrVZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}