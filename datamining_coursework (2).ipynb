{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UNuBe672UnLS",
        "ZvbALuMoUqzk",
        "Ef0g9dBeUw-e",
        "0syl2CUDkRY1",
        "nrknEUmtkvhJ",
        "kvNv4ycok1Bi",
        "7wX3FN1HFD5_",
        "OK8ZtHz_mnsD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FomSnGil9EcS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "mHsXkKhueT0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "rawdata_1 = pd.read_excel('/content/drive/MyDrive/Datamining/LFB_2019-22_south.xlsx')\n",
        "#original data is close to 400k rows\n",
        "#for my baroughb it is 18k\n",
        "meta_data1 = pd.read_excel('/content/drive/MyDrive/Datamining/LFB Metadata.xlsx')"
      ],
      "metadata": {
        "id": "Zac9ykANMRND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data load"
      ],
      "metadata": {
        "id": "UNuBe672UnLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#only cover numarical columns\n",
        "rawdata_1.describe()"
      ],
      "metadata": {
        "id": "B9ofTatxMaky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawdata_1.describe(include = 'O')"
      ],
      "metadata": {
        "id": "Fm83gJClhg87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get information about a method from offitial library\n",
        "rawdata_1.info()"
      ],
      "metadata": {
        "id": "T7WYS0HEky_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawdata_1[rawdata_1['CalYear']==2021].shape"
      ],
      "metadata": {
        "id": "onuMDceMhn6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#percantage of missing values\n",
        "print(\"percantage of missingvalues\")\n",
        "(rawdata_1.isna().sum().sort_values(ascending=False)/len(rawdata_1)*100).plot(kind='bar')"
      ],
      "metadata": {
        "id": "w5EwVA7hmiKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.imshow(rawdata_1.isnull(), cmap='hot', aspect='auto')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "HzI_Vrzyn406"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawdata_1.info()"
      ],
      "metadata": {
        "id": "qZdkkZRTNe5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change data type\n",
        "\n",
        "#if three time on value does not mean three time imporatant\n",
        "#it is catgrical\n",
        "\n",
        "rawdata_1['CalYear'] = rawdata_1['CalYear'].astype('O')\n",
        "rawdata_1['CalYear'].dtype # Changed .type to .dtype"
      ],
      "metadata": {
        "id": "Q-iXs4AQauaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawdata_1.columns"
      ],
      "metadata": {
        "id": "3Q-is8P5NF1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##columns"
      ],
      "metadata": {
        "id": "ZvbALuMoUqzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns1=['IncidentNumber', 'DateOfCall', 'CalYear', 'TimeOfCall', 'HourOfCall',\n",
        "       'IncidentGroup', 'StopCodeDescription', 'SpecialServiceType',\n",
        "       'PropertyCategory', 'PropertyType', 'AddressQualifier', 'Postcode_full',\n",
        "       'Postcode_district', 'UPRN', 'USRN', 'IncGeo_BoroughCode',\n",
        "       'IncGeo_BoroughName', 'ProperCase', 'IncGeo_WardCode',\n",
        "       'IncGeo_WardName', 'IncGeo_WardNameNew', 'Easting_m', 'Northing_m',\n",
        "       'Easting_rounded', 'Northing_rounded', 'Latitude', 'Longitude', 'FRS',\n",
        "       'IncidentStationGround', 'FirstPumpArriving_AttendanceTime',\n",
        "       'FirstPumpArriving_DeployedFromStation',\n",
        "       'SecondPumpArriving_AttendanceTime',\n",
        "       'SecondPumpArriving_DeployedFromStation',\n",
        "       'NumStationsWithPumpsAttending', 'NumPumpsAttending', 'PumpCount',\n",
        "       'PumpHoursRoundUp', 'Notional Cost (£)', 'NumCalls']"
      ],
      "metadata": {
        "id": "dDzfEmanSiW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dont display, too many values\n",
        "cat_columns2 = [\n",
        "    'IncidentNumber' ,\n",
        "    'DateOfCall',#Data of call\n",
        "    'TimeOfCall',#time of the call\n",
        "    'UPRN', #Unique Property Reference Number\n",
        "    'USRN', #Unique Street Reference Number\n",
        "    'Postcode_full',#full poste code\n",
        "    ]\n",
        "\n",
        "cat_columns1 = [\n",
        "    'CalYear','HourOfCall',\n",
        "    'IncidentGroup',#general type of insidante\n",
        "    'StopCodeDescription',#detaile type of incidante or false alarm\n",
        "    'SpecialServiceType',#if type was spacial service, what happened\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    'AddressQualifier',#does given adress same as fire\n",
        "    'Postcode_district',#post district\n",
        "    'IncGeo_WardCode', 'IncGeo_WardNameNew',#ward codes + name + new name\n",
        "    'IncidentStationGround',#fire brigaded area\n",
        "    'FirstPumpArriving_DeployedFromStation',#what station send first\n",
        "    'SecondPumpArriving_DeployedFromStation',#second pump location\n",
        "    'NumStationsWithPumpsAttending',#number of stations with pump\n",
        "    'NumPumpsAttending',#number of pumps attending\n",
        "    'PumpCount',#number of pumps\n",
        "    'NumCalls' #number of calls\n",
        "    ]\n",
        "\n",
        "drop_columns = [\n",
        "    #single value\n",
        "    #Borough Code #1 #Borough Name #1\n",
        "    'ProperCase',\n",
        "    'IncGeo_BoroughCode',\n",
        "    'IncGeo_BoroughName',\n",
        "    'FRS',#city name\n",
        "    #duplicate\n",
        "    'IncGeo_WardName',#ward name #we have new name\n",
        "    'Notional Cost (£)', #multipicaction of hours round up\n",
        "    ]\n",
        "    #repeted columns or single values\n",
        "\n",
        "num_columns1 = [\n",
        "    'Easting_m',#X converted from longitude\n",
        "    'Northing_m',#Y converted from lattitude\n",
        "    'Easting_rounded',#X rounded\n",
        "    'Northing_rounded',#Y rounded\n",
        "    'Latitude',#lattitude\n",
        "    'Longitude',#longtitude\n",
        "    'FirstPumpArriving_AttendanceTime',#arival time of first pump\n",
        "    'SecondPumpArriving_AttendanceTime',#arival time of second pump\n",
        "    'PumpHoursRoundUp',#pump hours\n",
        "    ]\n",
        "    #numrical values"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P-5xjP36UXhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in cat_columns1:\n",
        "  print(i)\n",
        "  print(rawdata_1[i].unique())\n",
        "for i in cat_columns2:\n",
        "  print(i)\n",
        "  print(rawdata_1[i].unique())"
      ],
      "metadata": {
        "id": "Zb2v0DT9hfCI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "lm = LabelEncoder\n",
        "data_2 = rawdata_1.copy()\n",
        "\n",
        "# Encoding categorical columns (optional, if needed)\n",
        "# for i in cat_columns1:\n",
        "#     data_2[i] = lm().fit_transform(data_2[i])\n",
        "\n",
        "# Define grid layout\n",
        "num_cols = 3  # Number of plots per row\n",
        "num_rows = (len(cat_columns1) + num_cols - 1) // num_cols  # Calculate number of rows\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 50))  # Adjust size as needed\n",
        "axes = axes.flatten()  # Flatten axes to easily index\n",
        "\n",
        "# Loop through columns and plot\n",
        "for idx, i in enumerate(cat_columns1):\n",
        "    data_2[i].value_counts().plot(kind='bar', ax=axes[idx])  # Assign plot to specific axis\n",
        "    axes[idx].set_xlabel(None)  # Hide x-axis label\n",
        "    axes[idx].set_ylabel(\"Count\")\n",
        "    axes[idx].set_title(f\"Bar plot {i}\")\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(idx + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3XGOipn2hXcO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_2[num_columns1].hist(figsize=(15, 12), bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oMG5_5JrTfas",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in clustring k mean is linner-\n",
        "#we cannt use catgrical becuase it assime it is numerical"
      ],
      "metadata": {
        "id": "y8dC6yWdtuqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 -- Use supervised learning (classification) to predict whether a call is likely to be a false alarm based on\n"
      ],
      "metadata": {
        "id": "q3XqZC25JIqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "#creat 3 datasets for three questions\n",
        "data_1 = rawdata_1.copy()\n",
        "#: Use supervised learning (classification) to predict whether a call is likely to be a false alarm based on\n",
        "col_ned = [\n",
        "    #call info\n",
        "    'DateOfCall',#Data of call\n",
        "    'TimeOfCall',#time of the call\n",
        "    # 'CalYear','HourOfCall',\n",
        "    'NumCalls', #number of calls\n",
        "\n",
        "    #output\n",
        "    'IncidentGroup',#general type of insidante\n",
        "    # 'StopCodeDescription',#detaile type of incidante or false alarm\n",
        "    #property info\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    #location info\n",
        "\n",
        "    'Postcode_district',#post district\n",
        "    'IncGeo_WardCode', 'IncGeo_WardNameNew',#ward codes + name + new name\n",
        "]\n",
        "\n",
        "col_2 = [\n",
        "    'Easting_rounded',\n",
        "    'Northing_rounded'\n",
        "    # 'Latitude',#lattitude\n",
        "    # 'Longitude',#longtitude\n",
        "    # 'Postcode_full',#full poste code\n",
        "]\n",
        "creat_col=[\n",
        "    #season of call\n",
        "    'Day','Month','Year',\n",
        "    'Hour','Minute','Second'\n",
        "\n",
        "    #history of the ward\n",
        "\n",
        "    #District\n",
        "]\n",
        "#\n",
        "data_1[col_ned[0]][1]"
      ],
      "metadata": {
        "id": "BQ4ydrMrDg9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data Cleaning"
      ],
      "metadata": {
        "id": "Ef0g9dBeUw-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot of Longitude vs Latitude\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data_1[\"Longitude\"], y=data_1[\"Latitude\"], alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"Geographical Mapping of Incidents\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lC4sly4HxsQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot of Longitude vs Latitude\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data_1[\"Easting_rounded\"], y=data_1[\"Northing_rounded\"], alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Easting_rounded\")\n",
        "plt.ylabel(\"Northing_rounded\")\n",
        "plt.title(\"Geographical Mapping of Incidents\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wsE1PXJ63MPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_1[['Easting_rounded','Northing_rounded']].describe()"
      ],
      "metadata": {
        "id": "s_Gbiwt13tvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_ = data_1.drop(data_1[(data_1['Easting_rounded'] > 550000) & (data_1['Northing_rounded'] > 200000)].index)\n",
        "data_ = data_.dropna(subset=['Easting_rounded', 'Northing_rounded'])\n",
        "data_.shape\n",
        "# data_ = data_1"
      ],
      "metadata": {
        "id": "6J0DDBDjx3Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rawdata_1[rawdata_1['Longitude']>-0.01 & rawdata_1['Latitude']<5]\n",
        "# data_ = data_1.drop(data_1[(data_1['Longitude'] > -0.01) & (data_1['Latitude'] < 5)].index)\n",
        "# data_ = data_.dropna(subset=['Longitude', 'Latitude'])\n",
        "#too manhy nulls\n",
        "# rawdata_1[(rawdata_1['Longitude']>-0.01) & (rawdata_1['Latitude']<5)][['Longitude','Latitude']]"
      ],
      "metadata": {
        "id": "Q418ilBP2xQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in col_ned[2:]:  # Assuming these are numerical columns\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=data_[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6BpncAzUkyDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q3 = data_['NumCalls'].quantile(0.999)\n",
        "if Q3<20: Q3=15\n",
        "data_ = data_[data_['NumCalls'] <= Q3]\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x=data_['NumCalls'])\n",
        "plt.title(f\"Boxplot of NumCalls\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_CRM22bLVAJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Maps"
      ],
      "metadata": {
        "id": "0syl2CUDkRY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import pyproj\n",
        "\n",
        "raw1 = 'NumCalls'\n",
        "unique_years = data_[raw1].unique()  # Get unique years\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate colors\n",
        "rows_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "# Define projection transformation (example: British National Grid to WGS 84)\n",
        "proj = pyproj.Transformer.from_crs(\"EPSG:27700\", \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "# Create the Folium map object before the loop\n",
        "m = folium.Map(location=[data_[\"Latitude\"].mean(), data_[\"Longitude\"].mean()], zoom_start=10) #added folium.Map object\n",
        "\n",
        "# Iterate over rows and create markers\n",
        "for _, row in data_.iterrows():\n",
        "    if pd.notna(row[\"Easting_rounded\"]) and pd.notna(row[\"Northing_rounded\"]):  # Check for NaN values\n",
        "        # Convert Easting/Northing to Latitude/Longitude\n",
        "        lon, lat = proj.transform(row[\"Easting_rounded\"], row[\"Northing_rounded\"])\n",
        "\n",
        "        row_ = row[raw1]  # Fix the indentation and variable reference\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[lat, lon],  # Use transformed coordinates\n",
        "            radius=1.5,\n",
        "            color=rows_color_map.get(row_, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=rows_color_map.get(row_, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m) #changed from .add_to(proj) to .add_to(m)\n",
        "m #displaying map instead of transformer"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U_M9OoTZ4iNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate a color map for years\n",
        "raw1 = 'NumCalls'\n",
        "unique_years = data_[raw1].unique()  # Get unique years\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate colors\n",
        "rows_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "# ['Easting_rounded', 'Northing_rounded']\n",
        "# Center map at the mean coordinates\n",
        "m = folium.Map(location=[data_[\"Latitude\"].mean(), data_[\"Longitude\"].mean()], zoom_start=13)\n",
        "\n",
        "# Add points to the map\n",
        "for _, row in data_.iterrows():\n",
        "    if pd.notna(row[\"Latitude\"]) and pd.notna(row[\"Longitude\"]):  # Check for NaN values\n",
        "          row_ = row[raw1]\n",
        "        # if row_ != unique_years[0]:\n",
        "          folium.CircleMarker(\n",
        "            location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "            radius=1.5,\n",
        "            color=rows_color_map.get(row_, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=rows_color_map.get(row_, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "          ).add_to(m)\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ],
      "metadata": {
        "id": "MM6EZLMvzhpQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install folium osmnx geopandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IVwC1-bsaLCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import osmnx as ox\n",
        "import geopandas as gpd\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Define the area (adjust to your city or region)\n",
        "place_name = \"London, UK\"  # Change this to your target city/region\n",
        "\n",
        "# Get road network (only major roads: motorways, primary, secondary, and trunk)\n",
        "road_types = [\"motorway\", \"primary\", \"secondary\", \"trunk\"]\n",
        "roads = ox.graph_from_place(place_name, network_type=\"drive\")\n",
        "\n",
        "# Convert to GeoDataFrame\n",
        "edges = ox.graph_to_gdfs(roads, nodes=False, edges=True)\n",
        "main_roads = edges[edges['highway'].apply(lambda x: isinstance(x, list) and any(r in x for r in road_types) or x in road_types)]\n",
        "\n",
        "# Create base map centered around incident locations\n",
        "map_center = [data_['Latitude'].mean(), data_['Longitude'].mean()]\n",
        "incident_map = folium.Map(location=map_center, zoom_start=12)\n",
        "\n",
        "# Add heatmap for incidents\n",
        "heat_data = list(zip(data_['Latitude'], data_['Longitude'], data_['NumCalls']))\n",
        "HeatMap(heat_data, radius=10).add_to(incident_map)\n",
        "\n",
        "# Add main roads to the map\n",
        "for _, road in main_roads.iterrows():\n",
        "    road_coords = list(road.geometry.coords)\n",
        "    folium.PolyLine(road_coords, color=\"blue\", weight=3, opacity=0.8).add_to(incident_map)\n",
        "\n",
        "# Show map\n",
        "incident_map\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L61IfrEXaIVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate a color map for years\n",
        "row1 = 'PropertyCategory'\n",
        "unique_years = sorted(data_[row1].dropna().unique())  # Get unique years and sort them\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate distinct colors\n",
        "year_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "\n",
        "# Center the map at the mean coordinates\n",
        "m = folium.Map(location=[data_[\"Latitude\"].mean(), data_[\"Longitude\"].mean()], zoom_start=10)\n",
        "\n",
        "# Add points to the map\n",
        "for _, row in data_.iterrows():\n",
        "    if pd.notna(row[\"Latitude\"]) and pd.notna(row[\"Longitude\"]):  # Check for NaN values\n",
        "        year = row[row1]\n",
        "        folium.CircleMarker(\n",
        "            location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "            radius=3,\n",
        "            color=year_color_map.get(year, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=year_color_map.get(year, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m)\n",
        "\n",
        "# Create a legend (color guide)\n",
        "legend_html = '''\n",
        "<div style=\"\n",
        "    position: fixed;\n",
        "    bottom: 50px; left: 50px; width: 200px; height: auto;\n",
        "    background-color: white; z-index:9999; font-size:14px;\n",
        "    padding: 10px; border-radius: 5px; box-shadow: 2px 2px 5px rgba(0,0,0,0.3);\n",
        "\">\n",
        "    <b>Year Color Guide</b><br>\n",
        "'''\n",
        "\n",
        "# Add each year to the legend with its corresponding color\n",
        "for year, color in year_color_map.items():\n",
        "    legend_html += f'<div style=\"display: flex; align-items: center;\"><div style=\"width: 15px; height: 15px; background-color: {color}; margin-right: 5px; border: 1px solid black;\"></div> {year}</div>'\n",
        "\n",
        "legend_html += \"</div>\"\n",
        "\n",
        "# Add the legend to the map\n",
        "m.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "# Add a title to the map\n",
        "title_html = '''\n",
        "<h3 align=\"center\" style=\"font-size:16px\"><b>Incident Map Colored by Year</b></h3>\n",
        "'''\n",
        "m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ],
      "metadata": {
        "id": "FAemMmTC2_sf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate a color map for years\n",
        "row1 = 'PropertyType'\n",
        "unique_years = sorted(data_[row1].dropna().unique())  # Get unique years and sort them\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate distinct colors\n",
        "year_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "\n",
        "# Center the map at the mean coordinates\n",
        "m = folium.Map(location=[data_[\"Latitude\"].mean(), data_[\"Longitude\"].mean()], zoom_start=13)\n",
        "\n",
        "# Add points to the map\n",
        "for _, row in data_.iterrows():\n",
        "    if pd.notna(row[\"Latitude\"]) and pd.notna(row[\"Longitude\"]):  # Check for NaN values\n",
        "        year = row[row1]\n",
        "        folium.CircleMarker(\n",
        "            location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "            radius=1.5,\n",
        "            color=year_color_map.get(year, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=year_color_map.get(year, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m)\n",
        "\n",
        "# Create a legend (color guide)\n",
        "legend_html = '''\n",
        "<div style=\"\n",
        "    position: fixed;\n",
        "    bottom: 50px; left: 50px; width: 200px; height: auto;\n",
        "    background-color: white; z-index:9999; font-size:14px;\n",
        "    padding: 10px; border-radius: 5px; box-shadow: 2px 2px 5px rgba(0,0,0,0.3);\n",
        "\">\n",
        "    <b>Year Color Guide</b><br>\n",
        "'''\n",
        "\n",
        "# Add each year to the legend with its corresponding color\n",
        "for year, color in year_color_map.items():\n",
        "    legend_html += f'<div style=\"display: flex; align-items: center;\"><div style=\"width: 15px; height: 15px; background-color: {color}; margin-right: 5px; border: 1px solid black;\"></div> {year}</div>'\n",
        "\n",
        "legend_html += \"</div>\"\n",
        "\n",
        "# Add the legend to the map\n",
        "m.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "# Add a title to the map\n",
        "title_html = '''\n",
        "<h3 align=\"center\" style=\"font-size:16px\"><b>Incident Map Colored by Year</b></h3>\n",
        "'''\n",
        "m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RdesirYIux2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creat data information\n",
        "data_[[\"Hour\", \"Minute\", \"Second\"]] = data_[\"TimeOfCall\"].str.split(\":\", expand=True)\n",
        "# Convert to datetime format\n",
        "data_[\"DateOfCall\"] = pd.to_datetime(data_[\"DateOfCall\"], format=\"%d %b %Y\")\n",
        "\n",
        "# Extract Day, Month, and Year\n",
        "data_[\"Day\"] = data_[\"DateOfCall\"].dt.day\n",
        "data_[\"Month\"] = data_[\"DateOfCall\"].dt.month\n",
        "data_[\"Year\"] = data_[\"DateOfCall\"].dt.year"
      ],
      "metadata": {
        "id": "hgE1a-XrKjQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_[creat_col[0]][1]\n",
        "# creat_col[0]\n",
        "# data_1[\"Day\"]"
      ],
      "metadata": {
        "id": "p9J-hqzkPfTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(data_[\"IncidentGroup\"], data_[\"HourOfCall\"])"
      ],
      "metadata": {
        "id": "TRjSApt8J6pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre model graphs"
      ],
      "metadata": {
        "id": "nrknEUmtkvhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# for i in creat_col:\n",
        "#   sns.violinplot(x=\"IncidentGroup\", y=i, data=data_1)\n",
        "#   plt.title(\"Boxplot of \"+i+\" by IncidentGroup\")\n",
        "#   plt.show()\n",
        "# Number of plots\n",
        "num_plots = len(creat_col)\n",
        "\n",
        "# Create subplots with 1 row and multiple columns\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))  # Adjust size\n",
        "\n",
        "# If only one column, axes won't be an array, so we wrap it\n",
        "if num_plots == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Loop through columns and create violin plots\n",
        "for i, col in enumerate(creat_col):\n",
        "    sns.violinplot(x=\"IncidentGroup\", y=col, data=data_, ax=axes[i])\n",
        "    axes[i].set_title(f\"Violin Plot of {col} by IncidentGroup\")\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mbyq8QGaJJo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['PropertyCategory',\n",
        "       'NumCalls','Postcode_district','IncGeo_WardCode']\n",
        "\n",
        "num_plots = len(col)\n",
        "\n",
        "# Create subplots with 1 row and multiple columns\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))  # Adjust size\n",
        "\n",
        "# If only one column, axes won't be an array, so we wrap it\n",
        "if num_plots == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Loop through columns and create violin plots\n",
        "for i, col in enumerate(col):\n",
        "    sns.violinplot(x=\"IncidentGroup\", y=col, data=data_, ax=axes[i])\n",
        "    axes[i].set_title(f\"Violin Plot of {col} by IncidentGroup\")\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Lb7sI_-SI5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_[col_ned].isnull().sum()\n",
        "data_.shape"
      ],
      "metadata": {
        "id": "wO6tbY3YkIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##scaling and getting data ready"
      ],
      "metadata": {
        "id": "kvNv4ycok1Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_.columns\n",
        "data_2 = data_[[\n",
        "        'IncidentGroup', 'PropertyCategory', #'PropertyType',\n",
        "        #'AddressQualifier',\n",
        "        'Postcode_district',\n",
        "        'IncGeo_WardCode', 'IncGeo_WardNameNew',\n",
        "        'Easting_rounded',\n",
        "        'Northing_rounded',\n",
        "        'NumCalls', 'Hour', #'Minute',\n",
        "        #'Second', 'Day',\n",
        "        'Month', #'Year'\n",
        "        ]]\n",
        "# Select categorical columns\n",
        "categorical_cols = [\n",
        "    'IncidentGroup', 'PropertyCategory', 'Hour','Month',\n",
        "    'Postcode_district', 'IncGeo_WardCode', 'IncGeo_WardNameNew'\n",
        "]\n",
        "\n",
        "# Apply one-hot encoding\n",
        "data_2 = pd.get_dummies(data_2, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Check the transformed dataset\n",
        "print(data_2.head())\n",
        "# data_2['PropertyCategory'].unique()"
      ],
      "metadata": {
        "id": "P7SLmKzaw99N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_2[['Easting_rounded','Northing_rounded']] = scaler.fit_transform(data_2[['Easting_rounded','Northing_rounded']])  # Use encoded data from one-hot encoding\n",
        "\n",
        "target = data_2[['IncidentGroup_Fire']]\n",
        "# data_2['target ']  = data_2['IncidentGroup_Fire']\n",
        "data_2= data_2.drop(columns=['IncidentGroup_Fire','IncidentGroup_Special Service','PropertyCategory_Non Residential'])\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(data_2)"
      ],
      "metadata": {
        "id": "fWf6x0Yvjt6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explained variance ratio (how much information each component keeps)\n",
        "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Find number of components for 90% variance\n",
        "percent_1 = 0.80\n",
        "num_components = np.argmax(explained_variance >= percent_1) + 1\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axhline(y=percent_1, color='r', linestyle='-')  # 90% threshold line\n",
        "plt.axvline(x=num_components, color='g', linestyle='--')  # Vertical line at chosen components\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Explained Variance vs. Number of Components\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDd_I7mCxrGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA with 30 components\n",
        "pca = PCA(n_components=30)\n",
        "data_com = pca.fit_transform(data_2)\n",
        "\n",
        "# Convert back to a DataFrame (optional, if needed)\n",
        "data_com = pd.DataFrame(data_com, columns=[f'PC{i+1}' for i in range(30)])\n",
        "\n",
        "# Check the new compressed data\n",
        "print(data_com.head())"
      ],
      "metadata": {
        "id": "b5LE9hgpyfXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target.value_counts()"
      ],
      "metadata": {
        "id": "PQMU132D5eJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelsing"
      ],
      "metadata": {
        "id": "KeTBqim0ykbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "rn = RandomForestClassifier(n_estimators=40, random_state=42)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "#imbalacne data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(data_com, target)\n",
        "#test train split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "#train model\n",
        "rn.fit(x_train, y_train)\n",
        "#valication\n",
        "y_pred = rn.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy valication part: {accuracy}\")\n",
        "# rn.feature_importances_"
      ],
      "metadata": {
        "id": "hxgKCA7cyjkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rn.predict(data_com)\n",
        "\n",
        "accuracy = accuracy_score(target, y_pred)\n",
        "print(f\"Accuracy overall: {accuracy}\")\n",
        "\n",
        "print(classification_report(target, y_pred))"
      ],
      "metadata": {
        "id": "GQKzLKNH5zje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Wrap Random Forest with a probability calibration method\n",
        "calibrated_model = CalibratedClassifierCV(rn, method=\"sigmoid\", cv=5)\n",
        "calibrated_model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "hxSYxen0SdJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get calibrated probabilities\n",
        "random_row = data_com.sample(n=1)  # Select one random row\n",
        "y_prob_calibrated = calibrated_model.predict_proba(random_row)\n",
        "print(y_prob_calibrated)"
      ],
      "metadata": {
        "id": "0UB4A7GDSssg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2- Supervised Regression Model: 1 - response time or  2 - Fire incident severity"
      ],
      "metadata": {
        "id": "n8B6k-6ZMMw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##intro"
      ],
      "metadata": {
        "id": "-xhoqL6oVoFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creat 3 datasets for three questions\n",
        "data2_1 = rawdata_1.copy()\n",
        "#: Use supervised learning (classification) to predict whether a call is likely to be a false alarm based on\n",
        "col_ned = [\n",
        "    #call info\n",
        "    'DateOfCall',#Data of call\n",
        "    'TimeOfCall',#time of the call\n",
        "    # 'CalYear','HourOfCall',\n",
        "    'NumCalls', #number of calls\n",
        "\n",
        "    #property info\n",
        "    #'IncidentGroup',#general type of incidante\n",
        "    'StopCodeDescription', #detaile type of incidante or false alarm\n",
        "    #'SpecialServiceType',\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    #'PropertyType', #info on type of building\n",
        "    'AddressQualifier', #was it correct address or nearby buiding\n",
        "    #location info\n",
        "    'IncGeo_WardCode',#ward codes#'IncGeo_WardNameNew',  + new name\n",
        "     #'Easting_m', 'Northing_m',\n",
        "    # 'Postcode_full', #full postcast + district\n",
        "    'Postcode_district', #post district\n",
        "    #target\n",
        "    #'IncidentStationGround',\n",
        "    'FirstPumpArriving_AttendanceTime',#in seconds\n",
        "    'FirstPumpArriving_DeployedFromStation',\n",
        "]\n",
        "col_num = [\n",
        "    'UPRN', #Unique Property Reference Number\n",
        "    'USRN', #Unique Street Reference Number\n",
        "    'Easting_rounded', 'Northing_rounded', #location on x and y\n",
        "    # 'Latitude', 'Longitude', #lattitude and lonitude\n",
        "        ]\n",
        "creat_col=[\n",
        "    #season of call\n",
        "    'Day','Month','Year',\n",
        "    'Hour','Minute','Second'\n",
        "]\n",
        "#Resource Allocation Factors: PropertyType, NumPumpsAttending, PumpHoursRoundUp\n",
        "data2_1[col_ned].isnull().sum()"
      ],
      "metadata": {
        "id": "lK-mFQX3MGL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate unique values per column and sum across columns\n",
        "for col in col_ned:\n",
        "    unique_values = data2_1[col].nunique()  # Get the number of unique values\n",
        "    print(f'{col} : {unique_values}')  # Print the column name and its unique value count\n",
        "#decided to swipe PropertyType with PropertyCategory becuse it was too detailed\n",
        "#and data on PropertyCategory would give us the meanfult enough details\n",
        "#futher info can be studied"
      ],
      "metadata": {
        "id": "WnMLYXuLCq48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data cleaning"
      ],
      "metadata": {
        "id": "O2nWAawzBrLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2_1.shape"
      ],
      "metadata": {
        "id": "o9p5Qti8yVvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot of Longitude vs Latitude\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data2_1[\"Easting_rounded\"], y=data2_1[\"Northing_rounded\"], alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Easting_rounded\")\n",
        "plt.ylabel(\"Northing_rounded\")\n",
        "plt.title(\"Geographical Mapping of Incidents\")\n",
        "plt.show()\n",
        "\n",
        "data2_2 = data2_1.drop(data_1[(data_1['Easting_rounded'] > 550000) & (data2_1['Northing_rounded'] > 200000)].index)\n",
        "# data2_2 = data2_1.dropna(subset=['Easting_rounded', 'Northing_rounded'])\n",
        "data2_2.shape\n",
        "# data_ = data_1"
      ],
      "metadata": {
        "id": "xbjk6rBYOeXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data2_2.shape)\n",
        "data2_2.dropna(subset=['FirstPumpArriving_AttendanceTime'], inplace=True)\n",
        "print(data2_2.shape)"
      ],
      "metadata": {
        "id": "LagsMosmoUgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d3 = data2_2[(data2_2['UPRN']<0.7)]#(0.25*(10^11)))]# & (data2_1['USRN']<60)\n",
        "d4 = data2_2[(data2_2['USRN']<10e+06)]#1.8*(10^7))]\n",
        "\n",
        "sns.boxplot(x=data2_2['USRN'])\n",
        "plt.show()\n",
        "data2_2['USRN'].describe()\n",
        "# sns.boxplot(x=data2_1['UPRN'])\n",
        "# plt.show()\n",
        "# data2_1['UPRN'].describe()\n",
        "print(f'Number of UPRN outlier {d3.shape}')\n",
        "print(f'Number of USRN outlier {d4.shape}')\n",
        "data2_2 = data2_2.drop(data2_2[(data2_2['USRN']<10e+06)].index)\n",
        " #cant use UPRN relaiably, too many 0 in it"
      ],
      "metadata": {
        "id": "Y3154cX0ocC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2_2.shape"
      ],
      "metadata": {
        "id": "un4M_qAcBBOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creat data information\n",
        "data2_2[[\"Hour\", \"Minute\", \"Second\"]] = data2_2[\"TimeOfCall\"].str.split(\":\", expand=True)\n",
        "# Convert to datetime format\n",
        "data2_2[\"DateOfCall\"] = pd.to_datetime(data2_2[\"DateOfCall\"], format=\"%d %b %Y\")\n",
        "\n",
        "# Extract Day, Month, and Year\n",
        "data2_2[\"Day\"] = data2_2[\"DateOfCall\"].dt.day\n",
        "data2_2[\"Month\"] = data2_2[\"DateOfCall\"].dt.month\n",
        "data2_2[\"Year\"] = data2_2[\"DateOfCall\"].dt.year"
      ],
      "metadata": {
        "id": "wSG9Q7PnILN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#handle outlier of number of calls\n",
        "Q3 = data2_2['NumCalls'].quantile(0.999)\n",
        "if Q3<20: Q3=15\n",
        "data2_2 = data2_2[data2_2['NumCalls'] <= Q3]\n",
        "# plt.figure(figsize=(6, 4))\n",
        "# sns.boxplot(x=data2_2['NumCalls'])\n",
        "# plt.title(f\"Boxplot of NumCalls\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "j_OauIbxJMVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pre graphs"
      ],
      "metadata": {
        "id": "7wX3FN1HFD5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in col_ned[2:]:  # Assuming these are numerical columns\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=data2_2[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()\n",
        "#we are going to ignore outlier of pump ariving time because it is out output\n",
        "#and they dont devate de results"
      ],
      "metadata": {
        "id": "Bj54G9klCbob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2_2[creat_col].dtypes\n",
        "# data2_2[creat_col] = data2_2[creat_col].astype('O')"
      ],
      "metadata": {
        "id": "Iou2xK_gOSZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# Number of plots\n",
        "# data2_2[creat_col] = data2_2[creat_col].astype('O')\n",
        "# df[\"Month\"] = df[\"Month\"].astype(int)\n",
        "# df[\"Day\"] = df[\"Day\"].astype(int)\n",
        "\n",
        "num_plots = len(creat_col)\n",
        "data2_2['arive'] = data2_2['FirstPumpArriving_AttendanceTime']/200\n",
        "# Create subplots with 1 row and multiple columns\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))  # Adjust size\n",
        "\n",
        "# If only one column, axes won't be an array, so we wrap it\n",
        "if num_plots == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Loop through columns and create violin plots\n",
        "for i, col in enumerate(creat_col[0:3]):\n",
        "    sns.violinplot(x=col, y=\"arive\", data=data2_2, ax=axes[i])\n",
        "    axes[i].set_title(f\"Violin Plot of {col} by First Pump Time\")\n",
        "for i, col in enumerate(creat_col[3:6]):\n",
        "    sns.violinplot(x=\"arive\", y=col, data=data2_2, ax=axes[i+3])\n",
        "    axes[i+3].set_title(f\"Violin Plot of {col} by First Pump Time\")\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hK_nOLriItjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = [\n",
        "    'NumCalls', #number of calls\n",
        "    #property info\n",
        "    #'IncidentGroup',#general type of incidante\n",
        "    'StopCodeDescription', #detaile type of incidante or false alarm\n",
        "    #'SpecialServiceType',\n",
        "    'PropertyCategory',#very detail info on type of building\n",
        "    #'PropertyType', #info on type of building\n",
        "    'AddressQualifier', #was it correct address or nearby buiding\n",
        "    #location info\n",
        "    'IncGeo_WardCode',#ward codes#'IncGeo_WardNameNew',  + new name\n",
        "     #'Easting_m', 'Northing_m',\n",
        "    # 'Postcode_full', #full postcast + district\n",
        "    'Postcode_district', #post district\n",
        "    ]\n",
        "\n",
        "num_plots = len(col)\n",
        "\n",
        "# Create subplots with 1 row and multiple columns\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))  # Adjust size\n",
        "\n",
        "# If only one column, axes won't be an array, so we wrap it\n",
        "if num_plots == 1:\n",
        "    axes = [axes]\n",
        "sns.violinplot(x=col[0], y=\"arive\", data=data2_2, ax=axes[0])\n",
        "axes[0].set_title(f\"Violin Plot of {col[0]} by IncidentGroup\")\n",
        "# Loop through columns and create violin plots\n",
        "for i, col in enumerate(col[1:]):\n",
        "    sns.violinplot(x=\"arive\", y=col, data=data2_2, ax=axes[i+1])\n",
        "    axes[i+1].set_title(f\"Violin Plot of {col} by IncidentGroup\")\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b83MVhF2P5k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##maps"
      ],
      "metadata": {
        "id": "OK8ZtHz_mnsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw1 = 'NumCalls'\n",
        "unique_years = data2_2[raw1].unique()  # Get unique years\n",
        "colors = sns.color_palette(\"husl\", len(unique_years)).as_hex()  # Generate colors\n",
        "rows_color_map = dict(zip(unique_years, colors))  # Map years to colors\n",
        "# Define projection transformation (example: British National Grid to WGS 84)\n",
        "proj = pyproj.Transformer.from_crs(\"EPSG:27700\", \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "# Create the Folium map object before the loop\n",
        "m = folium.Map(location=[data2_2[\"Latitude\"].mean(), data2_2[\"Longitude\"].mean()], zoom_start=10) #added folium.Map object\n",
        "\n",
        "# Iterate over rows and create markers\n",
        "for _, row in data2_2.iterrows():\n",
        "    if pd.notna(row[\"Easting_rounded\"]) and pd.notna(row[\"Northing_rounded\"]):  # Check for NaN values\n",
        "        # Convert Easting/Northing to Latitude/Longitude\n",
        "        lon, lat = proj.transform(row[\"Easting_rounded\"], row[\"Northing_rounded\"])\n",
        "\n",
        "        row_ = row[raw1]  # Fix the indentation and variable reference\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[lat, lon],  # Use transformed coordinates\n",
        "            radius=1.5,\n",
        "            color=rows_color_map.get(row_, \"black\"),  # Default to black if year is missing\n",
        "            fill=True,\n",
        "            fill_color=rows_color_map.get(row_, \"black\"),\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m) #changed from .add_to(proj) to .add_to(m)\n",
        "m #displaying map instead of transformer"
      ],
      "metadata": {
        "id": "s77VKmYbmm0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DATA preprocessing"
      ],
      "metadata": {
        "id": "MlscekH9E4dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_ned2 = [\n",
        "    'Day','Month','Year',\n",
        "    'Hour','Minute','Second',\n",
        "    'NumCalls', 'StopCodeDescription',\n",
        "    'PropertyCategory','AddressQualifier',\n",
        "    'IncGeo_WardCode', 'Postcode_district',\n",
        "    'Easting_rounded', 'Northing_rounded'\n",
        "]\n",
        "col_num = [\n",
        "    # 'UPRN', #Unique Property Reference Number\n",
        "    # 'USRN', #Unique Street Reference Number\n",
        "    'Easting_rounded',\n",
        "    'Northing_rounded'\n",
        "    ]\n",
        "#UPRN and USRN add no meaning to the calcumaction as they are not repated,\n",
        "#and they are just and indicator\n",
        "#and they have too many differnt values which would make the data nosy\n",
        "\n",
        "# data2_2[col_num].isnull().sum()\n",
        "data2_2[col_ned2].isnull().sum()\n",
        "\n",
        "data2_3 = data2_2[col_ned2].copy()\n",
        "\n",
        "# Calculate unique values per column and sum across columns\n",
        "for col in col_num:\n",
        "    unique_values = data2_3[col].nunique()  # Get the number of unique values\n",
        "    print(f'{col} : {unique_values}')  # Print the column name and its unique value count\n",
        "\n",
        "print(data2_2.shape)"
      ],
      "metadata": {
        "id": "OO6QLMxgBlj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data2_3.groupby(\"Northing_rounded\")[\"FirstPumpArriving_AttendanceTime\"].mean().sort_values()\n",
        "data2_3.columns\n",
        "data2_3.shape"
      ],
      "metadata": {
        "id": "fPY8SAFXUEJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "data2_3 = pd.get_dummies(data2_3, columns=col_ned2, drop_first=True)\n",
        "\n",
        "# Check the transformed dataset\n",
        "# print(data_2.head())\n",
        "# scaler = StandardScaler()\n",
        "# data_2[['Easting_rounded','Northing_rounded']] = scaler.fit_transform(data_2[['Easting_rounded','Northing_rounded']])  # Use encoded data from one-hot encoding\n",
        "\n",
        "target2 = data2_2['FirstPumpArriving_AttendanceTime'].copy()\n",
        "# data_2['target ']  = data_2['IncidentGroup_Fire']\n",
        "# data2_3= data2_3.drop(columns=['FirstPumpArriving_AttendanceTime'])\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(data2_3)"
      ],
      "metadata": {
        "id": "ay4u2qt_UrYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2_3.shape"
      ],
      "metadata": {
        "id": "0vKEpdH9gbSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explained variance ratio (how much information each component keeps)\n",
        "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Find number of components for 90% variance\n",
        "percent_1 = 0.80\n",
        "num_components = np.argmax(explained_variance >= percent_1) + 1\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axhline(y=percent_1, color='r', linestyle='-')  # 90% threshold line\n",
        "plt.axvline(x=num_components, color='g', linestyle='--')  # Vertical line at chosen components\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Explained Variance vs. Number of Components\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O6Q-uYfyXuJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA with 30 components\n",
        "pca2 = PCA(n_components=200)\n",
        "data2_com = pca2.fit_transform(data2_3)\n",
        "\n",
        "# Convert back to a DataFrame (optional, if needed)\n",
        "data2_com = pd.DataFrame(data2_com, columns=[f'PC{i+1}' for i in range(200)])\n",
        "\n",
        "# Check the new compressed data\n",
        "print(data2_com.shape)"
      ],
      "metadata": {
        "id": "hnYUCNQQX3Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##modeling"
      ],
      "metadata": {
        "id": "q8Bv2v8iYoll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Initialize the regression model\n",
        "rn2 = RandomForestRegressor(n_estimators=40, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(data2_com, target2, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rn2.fit(x_train, y_train)\n",
        "\n",
        "# Validation\n",
        "y_pred = rn2.predict(x_test)\n",
        "\n",
        "# Regression metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Validation MAE: {mae}\")\n",
        "print(f\"Validation RMSE: {rmse}\")\n",
        "print(f\"Validation R² Score: {r2}\")"
      ],
      "metadata": {
        "id": "NsCwf4ShYU40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on the full dataset\n",
        "y_pred_all = rn2.predict(data2_com)\n",
        "\n",
        "# Overall performance\n",
        "mae_all = mean_absolute_error(target2, y_pred_all)\n",
        "rmse_all = np.sqrt(mean_squared_error(target, y_pred_all))\n",
        "r2_all = r2_score(target, y_pred_all)\n",
        "\n",
        "print(f\"Overall MAE: {mae_all}\")\n",
        "print(f\"Overall RMSE: {rmse_all}\")\n",
        "print(f\"Overall R² Score: {r2_all}\")\n"
      ],
      "metadata": {
        "id": "hKrOM9EYY4yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pos processing : checking accrucy across wards"
      ],
      "metadata": {
        "id": "460HTUeNeVWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_wards = data2_2[\"IncGeo_WardName\"].unique()\n",
        "\n",
        "# # Select two specific wards (replace with actual names)\n",
        "# ward_1 = unique_wards[0]\n",
        "# ward_2 = unique_wards[1]\n",
        "\n",
        "# # Filter the dataset for each ward\n",
        "# df_ward1 = data2_2[data2_2[\"IncGeo_WardName\"] == ward_1]\n",
        "# df_ward2 = data2_2[data2_2[\"IncGeo_WardName\"] == ward_2]\n",
        "\n",
        "# X_ward1 = pd.get_dummies(X_ward1, columns=col_ned, drop_first=True)\n",
        "# X_ward2 = pd.get_dummies(X_ward2, columns=col_ned, drop_first=True)\n",
        "\n",
        "\n",
        "# #applying one hot encoding this way will relust in differnt numbero f clumns as we reduce one of the catagrical types\n",
        "# #thus making less culmns which will be problmadic with pca and model\n",
        "# X_ward1.columns"
      ],
      "metadata": {
        "id": "ljrtB7B6dqsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_wards = data2_2[\"IncGeo_WardCode\"].unique()\n",
        "df_ward1 = data2_2[col_ned2].copy()\n",
        "df_ward2 = pd.get_dummies(df_ward1, columns=col_ned2, drop_first=True)\n",
        "\n",
        "dy_ward1 = data2_2[\"FirstPumpArriving_AttendanceTime\"]\n",
        "\n",
        "# df_ward2 = pd.get_dummies(df_ward1, columns=col_ned2, drop_first=True)\n",
        "\n",
        "# Select two specific wards (replace with actual names)\n",
        "ward_1 = unique_wards[0]\n",
        "ward_2 = unique_wards[1]\n",
        "\n",
        "# Filter the dataset for each ward\n",
        "X_ward1 = df_ward2.loc[(df_ward1[\"IncGeo_WardCode\"] == ward_1), :]\n",
        "X_ward2 = df_ward2.loc[(df_ward1[\"IncGeo_WardCode\"] == ward_2), :]\n",
        "\n",
        "y_ward1 = dy_ward1[(df_ward1[\"IncGeo_WardCode\"] == ward_1)]\n",
        "y_ward2 = dy_ward1[(df_ward1[\"IncGeo_WardCode\"] == ward_2)]\n",
        "\n",
        "#applying one hot encoding this way will relust in differnt numbero f clumns as we reduce one of the catagrical types\n",
        "#thus making less culmns which will be problmadic with pca and model\n",
        "print(X_ward1.shape)\n",
        "# print(df_ward1.shape)"
      ],
      "metadata": {
        "id": "uBmT397HiSET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ward1 = pca2.transform(X_ward1)\n",
        "X_ward2 = pca2.transform(X_ward2)\n",
        "# Make predictions for both wards\n",
        "y_pred_ward1 = rn2.predict(X_ward1)\n",
        "y_pred_ward2 = rn2.predict(X_ward2)\n",
        "\n",
        "# Calculate error metrics for each ward\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
        "\n",
        "# Get results\n",
        "results_ward1 = evaluate_model(y_ward1, y_pred_ward1)\n",
        "results_ward2 = evaluate_model(y_ward2, y_pred_ward2)\n",
        "\n",
        "# Print the comparison\n",
        "print(f\"Performance in {ward_1}: {results_ward1}\")\n",
        "print(f\"Performance in {ward_2}: {results_ward2}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4LnII8vHgP0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model 3 : Identify hidden patterns in fire incidents to adjust safety policies, station resources, and response strategies"
      ],
      "metadata": {
        "id": "brS2uay4mtYp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EUwIN51msoR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}